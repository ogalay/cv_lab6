{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "IEU42WU0yiUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_mnv8yW2M0L",
        "outputId": "a3c8ec78-92c7-4103-a3cf-6151c0cdec31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import flow_transforms\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "d1ZD_KusrD_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Импорт файлов датасета"
      ],
      "metadata": {
        "id": "KkffPFH_qZz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMiMi1Mpph-w"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(file='./drive/MyDrive/dataset_chairs.zip') as zipnik:\n",
        "  zipnik.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Константы"
      ],
      "metadata": {
        "id": "NWV3rYMpuTjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value by which flow will be divided. Original value is 20 but 1 with\n",
        "# batchNorm gives good results\n",
        "DIV_FLOW = 20\n",
        "\n",
        "best_EPE = -1\n",
        "\n",
        "start_epoch = 0\n",
        "n_iter = start_epoch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "EPOCH_SIZE = 1000\n",
        "EPOCHS = 30\n",
        "LR = 10e-4\n",
        "MOMENTUM = 0.9\n",
        "BETA = 0.999\n",
        "WEIGHT_DECAY = 4e-4\n",
        "BIAS_DECAY = 0\n",
        "MULTISCALE_WEIGHTS = [0.005,0.01,0.02,0.08,0.32]\n",
        "PRINT_FREQ = 10\n",
        "MILESTONES = [10, 20]\n",
        "\n",
        "\n",
        "save_path = 'model_'"
      ],
      "metadata": {
        "id": "CY6Osb1-uWi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Запись прогресса обучения ?"
      ],
      "metadata": {
        "id": "NstBQgTiu-Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_writer = SummaryWriter(os.path.join(save_path,'train'))\n",
        "test_writer = SummaryWriter(os.path.join(save_path,'test'))\n",
        "output_writers = []\n",
        "for i in range(3):\n",
        "  output_writers.append(SummaryWriter(os.path.join(save_path,'test',str(i))))\n"
      ],
      "metadata": {
        "id": "veMMzzaDvBPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Преобразования для загрузки изображений датасета"
      ],
      "metadata": {
        "id": "4nA6CZtKty3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Data loading code\n",
        "input_transform = transforms.Compose([\n",
        "        flow_transforms.ArrayToTensor(),\n",
        "        transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n",
        "        transforms.Normalize(mean=[0.45,0.432,0.411], std=[1,1,1])\n",
        "    ])\n",
        "target_transform = transforms.Compose([\n",
        "        flow_transforms.ArrayToTensor(),\n",
        "        transforms.Normalize(mean=[0,0],std=[DIV_FLOW, DIV_FLOW])\n",
        "    ])"
      ],
      "metadata": {
        "id": "CoGv7r16t5ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создаём пайплайн [преобразований изображений](https://github.com/DimaKurd/FlowNetPytorch/blob/master/flow_transforms.py) для обучения"
      ],
      "metadata": {
        "id": "RtF0apyyqeia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "co_transform = flow_transforms.Compose([\n",
        "            flow_transforms.RandomTranslate(10),\n",
        "            flow_transforms.RandomRotate(10,5),\n",
        "            flow_transforms.RandomCrop((320,448)),\n",
        "            flow_transforms.RandomVerticalFlip(),\n",
        "            flow_transforms.RandomHorizontalFlip()\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "deGbm2vpqeB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Грузим данные, создаём даталоадеры"
      ],
      "metadata": {
        "id": "LlI7DH2iwQ7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import glob\n",
        "from listdataset import ListDataset\n",
        "from util_dataset import split2list\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def make_dataset(dir, split=None):\n",
        "    '''Will search for triplets that go by the pattern '[name]_img1.ppm  [name]_img2.ppm    [name]_flow.flo' '''\n",
        "    images = []\n",
        "    for flow_map in sorted(glob.glob(os.path.join(dir,'*_flow.flo'))):\n",
        "        flow_map = os.path.basename(flow_map)\n",
        "        root_filename = flow_map[:-9]\n",
        "        img1 = root_filename+'_img1.ppm'\n",
        "        img2 = root_filename+'_img2.ppm'\n",
        "        if not (os.path.isfile(os.path.join(dir,img1)) and os.path.isfile(os.path.join(dir,img2))):\n",
        "            continue\n",
        "\n",
        "        images.append([[img1,img2],flow_map])\n",
        "\n",
        "    return split2list(images, split, default_split=0.97)\n",
        "\n",
        "\n",
        "def flying_chairs(root, transform=None, target_transform=None,\n",
        "                  co_transform=None, split=None):\n",
        "    train_list, test_list = make_dataset(root,split)\n",
        "    train_dataset = ListDataset(root, train_list, transform, target_transform, co_transform)\n",
        "    test_dataset = ListDataset(root, test_list, transform, target_transform)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "8RXDkc85w0qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = flying_chairs(\n",
        "        'dataset',\n",
        "        transform=input_transform,\n",
        "        target_transform=target_transform,\n",
        "        co_transform=co_transform,\n",
        "        split=TRAIN_RATIO\n",
        "    )"
      ],
      "metadata": {
        "id": "R3vIo22_vl7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train len: {len(train_set)}. Test len {len(test_set)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2M8rjwmvpxl",
        "outputId": "563f1f2d-8f30-4323-92c8-ff7189dbb204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train len: 1228. Test len 267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        test_set,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        shuffle=False)\n"
      ],
      "metadata": {
        "id": "2cAWts8RzIHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Объявление модели"
      ],
      "metadata": {
        "id": "zLnU2wHq4Kru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_, constant_\n",
        "from util_model import conv, predict_flow, deconv, crop_like\n",
        "\n",
        "class FlowNetS(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,batchNorm=True):\n",
        "        super(FlowNetS,self).__init__()\n",
        "\n",
        "        self.batchNorm = batchNorm\n",
        "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n",
        "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n",
        "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n",
        "        self.conv3_1 = conv(self.batchNorm, 256,  256)\n",
        "        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n",
        "        self.conv4_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n",
        "        self.conv5_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n",
        "        self.conv6_1 = conv(self.batchNorm,1024, 1024)\n",
        "\n",
        "        self.deconv5 = deconv(1024,512)\n",
        "        self.deconv4 = deconv(1026,256)\n",
        "        self.deconv3 = deconv(770,128)\n",
        "        self.deconv2 = deconv(386,64)\n",
        "\n",
        "        self.predict_flow6 = predict_flow(1024)\n",
        "        self.predict_flow5 = predict_flow(1026)\n",
        "        self.predict_flow4 = predict_flow(770)\n",
        "        self.predict_flow3 = predict_flow(386)\n",
        "        self.predict_flow2 = predict_flow(194)\n",
        "\n",
        "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                kaiming_normal_(m.weight, 0.1)\n",
        "                if m.bias is not None:\n",
        "                    constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                constant_(m.weight, 1)\n",
        "                constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_conv2 = self.conv2(self.conv1(x))\n",
        "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
        "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
        "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
        "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
        "\n",
        "        flow6       = self.predict_flow6(out_conv6)\n",
        "        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n",
        "        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n",
        "\n",
        "        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
        "        flow5       = self.predict_flow5(concat5)\n",
        "        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n",
        "        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n",
        "\n",
        "        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
        "        flow4       = self.predict_flow4(concat4)\n",
        "        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n",
        "        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n",
        "\n",
        "        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n",
        "        flow3       = self.predict_flow3(concat3)\n",
        "        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n",
        "        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n",
        "\n",
        "        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n",
        "        flow2 = self.predict_flow2(concat2)\n",
        "\n",
        "        if self.training:\n",
        "            return flow2,flow3,flow4,flow5,flow6\n",
        "        else:\n",
        "            return flow2\n",
        "\n",
        "    def weight_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
        "\n",
        "    def bias_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'bias' in name]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGe1Lki84UFU",
        "outputId": "99a0b780-0587-47ba-93d0-0fafa416b198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/util_model.py:10: ImportWarning: failed to load custom correlation modulewhich is needed for FlowNetC\n",
            "  warnings.warn(\"failed to load custom correlation module\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Инициализируем модель"
      ],
      "metadata": {
        "id": "iyTteUUC5Js3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FlowNetS(batchNorm=False).to(device)"
      ],
      "metadata": {
        "id": "qEDsxTIm5Op5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_groups = [{'params': model.bias_parameters(), 'weight_decay': BIAS_DECAY},\n",
        "                {'params': model.weight_parameters(), 'weight_decay': WEIGHT_DECAY}]\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "        cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "gWVWx1eo5ot8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(param_groups, LR, betas=(MOMENTUM, BETA))"
      ],
      "metadata": {
        "id": "ZYuER1lY5-Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=0.5)"
      ],
      "metadata": {
        "id": "aBhoFnBg6vE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели"
      ],
      "metadata": {
        "id": "caT4_1Sq7HmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from util import flow2rgb, AverageMeter, save_checkpoint\n",
        "from multiscaleloss import multiscaleEPE, realEPE\n",
        "\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, train_writer):\n",
        "    global n_iter\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    epoch_size = len(train_loader)\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        # if args.sparse:\n",
        "        #     # Since Target pooling is not very precise when sparse,\n",
        "        #     # take the highest resolution prediction and upsample it instead of downsampling target\n",
        "        #     h, w = target.size()[-2:]\n",
        "        #     output = [F.interpolate(output[0], (h,w)), *output[1:]]\n",
        "\n",
        "        loss = multiscaleEPE(output, target, weights=MULTISCALE_WEIGHTS, sparse=False)\n",
        "        flow2_EPE = DIV_FLOW * realEPE(output[0], target, sparse=False)\n",
        "        # record loss and EPE\n",
        "        losses.update(loss.item(), target.size(0))\n",
        "        train_writer.add_scalar('train_loss', loss.item(), n_iter)\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # compute gradient and do optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t Time {3}\\t Data {4}\\t Loss {5}\\t EPE {6}'\n",
        "                  .format(epoch, i, epoch_size, batch_time,\n",
        "                          data_time, losses, flow2_EPEs))\n",
        "        n_iter += 1\n",
        "        if i >= epoch_size:\n",
        "            break\n",
        "\n",
        "    return losses.avg, flow2_EPEs.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, epoch, output_writers):\n",
        "\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        flow2_EPE = DIV_FLOW*realEPE(output, target, sparse=False)\n",
        "        # record EPE\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i < len(output_writers):  # log first output of first batches\n",
        "            if epoch == start_epoch:\n",
        "                mean_values = torch.tensor([0.45,0.432,0.411], dtype=input.dtype).view(3,1,1)\n",
        "                output_writers[i].add_image('GroundTruth', flow2rgb(DIV_FLOW * target[0], max_value=10), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,:3].cpu() + mean_values).clamp(0,1), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,3:].cpu() + mean_values).clamp(0,1), 1)\n",
        "            output_writers[i].add_image('FlowNet Outputs', flow2rgb(DIV_FLOW * output[0], max_value=10), epoch)\n",
        "\n",
        "        if i % PRINT_FREQ == 0:\n",
        "            print('Test: [{0}/{1}]\\t Time {2}\\t EPE {3}'\n",
        "                  .format(i, len(val_loader), batch_time, flow2_EPEs))\n",
        "\n",
        "    print(' * EPE {:.3f}'.format(flow2_EPEs.avg))\n",
        "\n",
        "    return flow2_EPEs.avg\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(n_iter, EPOCHS):\n",
        "        scheduler.step()\n",
        "\n",
        "        # train for one epoch\n",
        "        train_loss, train_EPE = train(train_loader, model, optimizer, epoch, train_writer)\n",
        "        train_writer.add_scalar('mean EPE', train_EPE, epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "\n",
        "        with torch.no_grad():\n",
        "            EPE = validate(val_loader, model, epoch, output_writers)\n",
        "        test_writer.add_scalar('mean EPE', EPE, epoch)\n",
        "\n",
        "        if best_EPE < 0:\n",
        "            best_EPE = EPE\n",
        "\n",
        "        is_best = EPE < best_EPE\n",
        "        best_EPE = min(EPE, best_EPE)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'arch': 'FlowNetS',\n",
        "            'state_dict': model.module.state_dict(),\n",
        "            'best_EPE': best_EPE,\n",
        "            'div_flow': DIV_FLOW\n",
        "        }, is_best, save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77Ag_3io7Dca",
        "outputId": "7ca67f3a-67cd-4597-f3a9-0829757fade3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/154]\t Time 20.022 (20.022)\t Data 3.073 (3.073)\t Loss 109.026 (109.026)\t EPE 22.229 (22.229)\n",
            "Epoch: [0][10/154]\t Time 0.241 (2.943)\t Data 0.000 (1.258)\t Loss 345.219 (615.874)\t EPE 16.695 (34.599)\n",
            "Epoch: [0][20/154]\t Time 0.259 (2.374)\t Data 0.000 (1.410)\t Loss 296.462 (411.514)\t EPE 17.475 (27.403)\n",
            "Epoch: [0][30/154]\t Time 0.238 (2.193)\t Data 0.000 (1.480)\t Loss 153.376 (379.840)\t EPE 17.500 (25.467)\n",
            "Epoch: [0][40/154]\t Time 0.211 (2.066)\t Data 0.005 (1.485)\t Loss 1660.138 (657.372)\t EPE 26.589 (28.538)\n",
            "Epoch: [0][50/154]\t Time 0.254 (2.003)\t Data 0.000 (1.500)\t Loss 5822.542 (1030.561)\t EPE 140.532 (32.478)\n",
            "Epoch: [0][60/154]\t Time 0.236 (1.988)\t Data 0.000 (1.537)\t Loss 15088.424 (5669.139)\t EPE 302.290 (120.543)\n",
            "Epoch: [0][70/154]\t Time 0.243 (2.003)\t Data 0.000 (1.589)\t Loss 1665893.250 (37391.799)\t EPE 38892.789 (765.628)\n",
            "Epoch: [0][80/154]\t Time 0.240 (1.973)\t Data 0.004 (1.588)\t Loss 96297.312 (60697.474)\t EPE 499.145 (926.225)\n",
            "Epoch: [0][90/154]\t Time 0.241 (1.942)\t Data 0.002 (1.581)\t Loss 246259.844 (66203.014)\t EPE 1726.244 (908.242)\n",
            "Epoch: [0][100/154]\t Time 0.258 (1.916)\t Data 0.000 (1.573)\t Loss 18405476.000 (632771.053)\t EPE 244331.781 (15832.710)\n",
            "Epoch: [0][110/154]\t Time 0.258 (1.893)\t Data 0.000 (1.565)\t Loss 818352.250 (1328564.029)\t EPE 41558.102 (48954.749)\n",
            "Epoch: [0][120/154]\t Time 0.222 (1.875)\t Data 0.000 (1.560)\t Loss 1200277.875 (1290294.171)\t EPE 33164.723 (49058.464)\n",
            "Epoch: [0][130/154]\t Time 0.232 (1.857)\t Data 0.005 (1.553)\t Loss 237938.703 (1222638.877)\t EPE 6298.046 (45995.497)\n",
            "Epoch: [0][140/154]\t Time 0.241 (1.847)\t Data 0.002 (1.554)\t Loss 100244.219 (1152543.539)\t EPE 1104.627 (43166.868)\n",
            "Epoch: [0][150/154]\t Time 0.374 (1.835)\t Data 0.310 (1.556)\t Loss 47971.414 (1080309.971)\t EPE 1078.041 (40405.993)\n",
            "Test: [0/34]\t Time 5.186 (5.186)\t EPE 325.270 (325.270)\n",
            "Test: [10/34]\t Time 0.153 (0.698)\t EPE 266.862 (447.097)\n",
            "Test: [20/34]\t Time 0.270 (0.526)\t EPE 401.680 (459.969)\n",
            "Test: [30/34]\t Time 0.289 (0.474)\t EPE 472.053 (499.341)\n",
            " * EPE 500.655\n",
            "Epoch: [1][0/154]\t Time 3.110 (3.110)\t Data 2.945 (2.945)\t Loss 22924.049 (22924.049)\t EPE 516.397 (516.397)\n",
            "Epoch: [1][10/154]\t Time 2.259 (1.786)\t Data 2.142 (1.659)\t Loss 9447.181 (13519.677)\t EPE 178.875 (276.003)\n",
            "Epoch: [1][20/154]\t Time 2.013 (1.729)\t Data 1.903 (1.614)\t Loss 3243.299 (9576.266)\t EPE 94.114 (206.400)\n",
            "Epoch: [1][30/154]\t Time 2.114 (1.729)\t Data 1.960 (1.616)\t Loss 60206.242 (20364.719)\t EPE 871.492 (683.352)\n",
            "Epoch: [1][40/154]\t Time 1.581 (1.807)\t Data 1.488 (1.696)\t Loss 39357.277 (25457.030)\t EPE 698.088 (705.508)\n",
            "Epoch: [1][50/154]\t Time 1.555 (1.780)\t Data 1.486 (1.674)\t Loss 13544.931 (24117.443)\t EPE 304.338 (642.876)\n",
            "Epoch: [1][60/154]\t Time 1.567 (1.766)\t Data 1.503 (1.663)\t Loss 7357.271 (21693.348)\t EPE 163.831 (575.615)\n",
            "Epoch: [1][70/154]\t Time 2.129 (1.759)\t Data 2.017 (1.659)\t Loss 2398.380 (19176.686)\t EPE 58.633 (507.223)\n",
            "Epoch: [1][80/154]\t Time 2.449 (1.752)\t Data 2.325 (1.651)\t Loss 2130.694 (17088.525)\t EPE 61.246 (452.182)\n",
            "Epoch: [1][90/154]\t Time 3.195 (1.745)\t Data 3.065 (1.643)\t Loss 2227.833 (15432.199)\t EPE 64.130 (408.584)\n",
            "Epoch: [1][100/154]\t Time 3.869 (1.739)\t Data 3.748 (1.638)\t Loss 2078.105 (14092.694)\t EPE 63.387 (373.721)\n",
            "Epoch: [1][110/154]\t Time 4.737 (1.731)\t Data 4.561 (1.630)\t Loss 1265.278 (12944.380)\t EPE 45.496 (344.113)\n",
            "Epoch: [1][120/154]\t Time 4.715 (1.727)\t Data 4.588 (1.620)\t Loss 963.758 (11970.579)\t EPE 35.711 (318.971)\n",
            "Epoch: [1][130/154]\t Time 4.418 (1.722)\t Data 4.270 (1.610)\t Loss 962.685 (11147.215)\t EPE 40.020 (298.059)\n",
            "Epoch: [1][140/154]\t Time 4.171 (1.718)\t Data 4.018 (1.601)\t Loss 1796.654 (10422.383)\t EPE 62.428 (279.614)\n",
            "Epoch: [1][150/154]\t Time 3.959 (1.720)\t Data 3.825 (1.599)\t Loss 1056.438 (9786.523)\t EPE 36.455 (263.341)\n",
            "Test: [0/34]\t Time 0.735 (0.735)\t EPE 45.596 (45.596)\n",
            "Test: [10/34]\t Time 0.327 (0.297)\t EPE 27.172 (37.095)\n",
            "Test: [20/34]\t Time 0.350 (0.272)\t EPE 29.762 (37.130)\n",
            "Test: [30/34]\t Time 0.508 (0.266)\t EPE 55.793 (35.566)\n",
            " * EPE 36.063\n",
            "Epoch: [2][0/154]\t Time 4.259 (4.259)\t Data 4.098 (4.098)\t Loss 848.562 (848.562)\t EPE 36.358 (36.358)\n",
            "Epoch: [2][10/154]\t Time 3.097 (1.882)\t Data 2.973 (1.708)\t Loss 638.307 (879.528)\t EPE 31.897 (37.311)\n",
            "Epoch: [2][20/154]\t Time 4.611 (2.071)\t Data 4.480 (1.888)\t Loss 777.372 (869.444)\t EPE 32.530 (35.841)\n",
            "Epoch: [2][30/154]\t Time 3.811 (1.930)\t Data 3.640 (1.747)\t Loss 690.742 (809.783)\t EPE 26.807 (33.835)\n",
            "Epoch: [2][40/154]\t Time 2.712 (1.841)\t Data 2.583 (1.658)\t Loss 653.984 (798.320)\t EPE 26.472 (32.934)\n",
            "Epoch: [2][50/154]\t Time 2.528 (1.811)\t Data 2.407 (1.629)\t Loss 469.121 (759.052)\t EPE 24.136 (31.700)\n",
            "Epoch: [2][60/154]\t Time 2.596 (1.794)\t Data 2.486 (1.615)\t Loss 413.557 (735.175)\t EPE 19.965 (31.431)\n",
            "Epoch: [2][70/154]\t Time 2.497 (1.783)\t Data 2.387 (1.605)\t Loss 564.855 (706.159)\t EPE 35.645 (30.964)\n",
            "Epoch: [2][80/154]\t Time 2.467 (1.778)\t Data 2.348 (1.599)\t Loss 868.636 (694.127)\t EPE 42.814 (30.883)\n",
            "Epoch: [2][90/154]\t Time 2.554 (1.770)\t Data 2.442 (1.591)\t Loss 577.193 (675.416)\t EPE 27.867 (30.431)\n",
            "Epoch: [2][100/154]\t Time 2.605 (1.765)\t Data 2.487 (1.586)\t Loss 542.188 (676.005)\t EPE 23.634 (30.270)\n",
            "Epoch: [2][110/154]\t Time 2.580 (1.760)\t Data 2.467 (1.582)\t Loss 1039.394 (662.069)\t EPE 38.884 (29.959)\n",
            "Epoch: [2][120/154]\t Time 2.480 (1.754)\t Data 2.369 (1.576)\t Loss 586.654 (641.775)\t EPE 29.658 (29.407)\n",
            "Epoch: [2][130/154]\t Time 2.493 (1.749)\t Data 2.374 (1.571)\t Loss 1344.396 (641.463)\t EPE 40.286 (29.526)\n",
            "Epoch: [2][140/154]\t Time 2.555 (1.744)\t Data 2.437 (1.566)\t Loss 358.207 (648.086)\t EPE 22.831 (29.360)\n",
            "Epoch: [2][150/154]\t Time 2.572 (1.743)\t Data 2.446 (1.568)\t Loss 587.393 (648.995)\t EPE 19.196 (28.959)\n",
            "Test: [0/34]\t Time 0.823 (0.823)\t EPE 29.281 (29.281)\n",
            "Test: [10/34]\t Time 0.287 (0.261)\t EPE 18.122 (22.155)\n",
            "Test: [20/34]\t Time 0.439 (0.236)\t EPE 21.593 (21.820)\n",
            "Test: [30/34]\t Time 0.576 (0.268)\t EPE 30.815 (20.591)\n",
            " * EPE 20.787\n",
            "Epoch: [3][0/154]\t Time 5.552 (5.552)\t Data 5.398 (5.398)\t Loss 496.652 (496.652)\t EPE 18.876 (18.876)\n",
            "Epoch: [3][10/154]\t Time 2.478 (2.023)\t Data 2.353 (1.882)\t Loss 780.067 (478.353)\t EPE 37.534 (23.484)\n",
            "Epoch: [3][20/154]\t Time 2.446 (1.848)\t Data 2.335 (1.713)\t Loss 470.621 (486.581)\t EPE 29.446 (24.844)\n",
            "Epoch: [3][30/154]\t Time 2.500 (1.794)\t Data 2.365 (1.658)\t Loss 567.352 (464.737)\t EPE 27.317 (24.542)\n",
            "Epoch: [3][40/154]\t Time 2.381 (1.763)\t Data 2.255 (1.615)\t Loss 264.923 (442.119)\t EPE 24.740 (24.236)\n",
            "Epoch: [3][50/154]\t Time 2.423 (1.749)\t Data 2.312 (1.601)\t Loss 358.567 (431.957)\t EPE 21.111 (23.911)\n",
            "Epoch: [3][60/154]\t Time 2.744 (1.747)\t Data 2.624 (1.601)\t Loss 256.929 (419.861)\t EPE 19.480 (23.575)\n",
            "Epoch: [3][70/154]\t Time 2.907 (1.744)\t Data 2.797 (1.593)\t Loss 992.273 (438.531)\t EPE 40.382 (24.203)\n",
            "Epoch: [3][80/154]\t Time 2.994 (1.740)\t Data 2.874 (1.584)\t Loss 336.870 (456.074)\t EPE 20.568 (24.754)\n",
            "Epoch: [3][90/154]\t Time 3.576 (1.734)\t Data 3.461 (1.575)\t Loss 309.117 (452.247)\t EPE 20.858 (24.826)\n",
            "Epoch: [3][100/154]\t Time 4.242 (1.731)\t Data 4.119 (1.570)\t Loss 730.600 (449.976)\t EPE 38.182 (24.767)\n",
            "Epoch: [3][110/154]\t Time 4.449 (1.720)\t Data 4.312 (1.558)\t Loss 231.444 (444.462)\t EPE 18.995 (24.572)\n",
            "Epoch: [3][120/154]\t Time 4.207 (1.709)\t Data 4.087 (1.546)\t Loss 423.703 (441.445)\t EPE 25.482 (24.461)\n",
            "Epoch: [3][130/154]\t Time 3.102 (1.694)\t Data 2.962 (1.534)\t Loss 176.310 (429.512)\t EPE 15.302 (24.219)\n",
            "Epoch: [3][140/154]\t Time 2.435 (1.686)\t Data 2.314 (1.530)\t Loss 143.458 (421.292)\t EPE 15.743 (24.171)\n",
            "Epoch: [3][150/154]\t Time 2.268 (1.684)\t Data 2.159 (1.531)\t Loss 238.497 (409.834)\t EPE 19.594 (23.896)\n",
            "Test: [0/34]\t Time 1.533 (1.533)\t EPE 24.500 (24.500)\n",
            "Test: [10/34]\t Time 0.631 (0.492)\t EPE 16.538 (18.449)\n",
            "Test: [20/34]\t Time 0.681 (0.436)\t EPE 17.932 (18.053)\n",
            "Test: [30/34]\t Time 0.238 (0.380)\t EPE 24.201 (16.936)\n",
            " * EPE 17.104\n",
            "Epoch: [4][0/154]\t Time 4.418 (4.418)\t Data 4.217 (4.217)\t Loss 351.979 (351.979)\t EPE 25.147 (25.147)\n",
            "Epoch: [4][10/154]\t Time 2.895 (1.792)\t Data 2.776 (1.650)\t Loss 181.881 (269.701)\t EPE 19.708 (20.936)\n",
            "Epoch: [4][20/154]\t Time 2.182 (1.691)\t Data 2.070 (1.571)\t Loss 278.595 (273.860)\t EPE 16.857 (21.137)\n",
            "Epoch: [4][30/154]\t Time 2.022 (1.675)\t Data 1.912 (1.566)\t Loss 187.237 (265.946)\t EPE 18.328 (21.050)\n",
            "Epoch: [4][40/154]\t Time 1.698 (1.662)\t Data 1.620 (1.557)\t Loss 178.957 (256.420)\t EPE 19.707 (21.037)\n",
            "Epoch: [4][50/154]\t Time 2.037 (1.660)\t Data 1.920 (1.559)\t Loss 179.006 (253.080)\t EPE 17.447 (21.101)\n",
            "Epoch: [4][60/154]\t Time 1.688 (1.652)\t Data 1.617 (1.552)\t Loss 241.418 (251.418)\t EPE 20.485 (21.063)\n",
            "Epoch: [4][70/154]\t Time 1.745 (1.648)\t Data 1.671 (1.551)\t Loss 197.584 (248.845)\t EPE 24.213 (21.164)\n",
            "Epoch: [4][80/154]\t Time 2.394 (1.648)\t Data 2.278 (1.553)\t Loss 151.908 (245.128)\t EPE 18.295 (21.137)\n",
            "Epoch: [4][90/154]\t Time 3.075 (1.641)\t Data 2.948 (1.547)\t Loss 133.349 (239.615)\t EPE 18.288 (20.864)\n",
            "Epoch: [4][100/154]\t Time 3.420 (1.631)\t Data 3.290 (1.538)\t Loss 211.941 (236.780)\t EPE 21.900 (20.685)\n",
            "Epoch: [4][110/154]\t Time 3.345 (1.630)\t Data 3.184 (1.530)\t Loss 248.071 (233.813)\t EPE 22.070 (20.540)\n",
            "Epoch: [4][120/154]\t Time 2.527 (1.626)\t Data 2.406 (1.520)\t Loss 149.627 (233.481)\t EPE 16.093 (20.507)\n",
            "Epoch: [4][130/154]\t Time 2.498 (1.630)\t Data 2.378 (1.518)\t Loss 195.553 (232.701)\t EPE 20.670 (20.331)\n",
            "Epoch: [4][140/154]\t Time 2.470 (1.634)\t Data 2.347 (1.517)\t Loss 152.808 (232.168)\t EPE 16.944 (20.359)\n",
            "Epoch: [4][150/154]\t Time 2.463 (1.661)\t Data 2.327 (1.540)\t Loss 156.442 (232.206)\t EPE 21.011 (20.366)\n",
            "Test: [0/34]\t Time 1.127 (1.127)\t EPE 24.045 (24.045)\n",
            "Test: [10/34]\t Time 0.343 (0.372)\t EPE 16.741 (18.092)\n",
            "Test: [20/34]\t Time 0.253 (0.286)\t EPE 21.682 (18.075)\n",
            "Test: [30/34]\t Time 0.280 (0.259)\t EPE 22.213 (17.135)\n",
            " * EPE 17.201\n",
            "Epoch: [5][0/154]\t Time 2.877 (2.877)\t Data 2.745 (2.745)\t Loss 132.693 (132.693)\t EPE 16.886 (16.886)\n",
            "Epoch: [5][10/154]\t Time 2.290 (1.741)\t Data 2.177 (1.627)\t Loss 173.665 (219.358)\t EPE 21.125 (20.895)\n",
            "Epoch: [5][20/154]\t Time 2.227 (1.691)\t Data 2.105 (1.584)\t Loss 183.967 (229.671)\t EPE 21.140 (20.397)\n",
            "Epoch: [5][30/154]\t Time 2.363 (1.671)\t Data 2.241 (1.568)\t Loss 215.007 (227.810)\t EPE 18.088 (20.897)\n",
            "Epoch: [5][40/154]\t Time 2.463 (1.668)\t Data 2.353 (1.559)\t Loss 130.183 (226.486)\t EPE 16.937 (20.557)\n",
            "Epoch: [5][50/154]\t Time 2.783 (1.666)\t Data 2.672 (1.544)\t Loss 212.376 (220.092)\t EPE 20.142 (20.397)\n",
            "Epoch: [5][60/154]\t Time 3.704 (1.671)\t Data 3.594 (1.540)\t Loss 116.027 (209.956)\t EPE 15.419 (20.136)\n",
            "Epoch: [5][70/154]\t Time 3.873 (1.669)\t Data 3.756 (1.531)\t Loss 165.721 (200.967)\t EPE 21.737 (20.026)\n",
            "Epoch: [5][80/154]\t Time 4.428 (1.662)\t Data 4.304 (1.519)\t Loss 122.775 (199.041)\t EPE 15.071 (20.002)\n",
            "Epoch: [5][90/154]\t Time 4.262 (1.654)\t Data 4.102 (1.509)\t Loss 179.455 (198.056)\t EPE 23.478 (19.852)\n",
            "Epoch: [5][100/154]\t Time 2.992 (1.641)\t Data 2.862 (1.498)\t Loss 108.562 (194.485)\t EPE 16.102 (19.803)\n",
            "Epoch: [5][110/154]\t Time 2.047 (1.635)\t Data 1.972 (1.496)\t Loss 137.629 (191.289)\t EPE 17.935 (19.652)\n",
            "Epoch: [5][120/154]\t Time 1.759 (1.636)\t Data 1.684 (1.502)\t Loss 162.870 (189.043)\t EPE 19.514 (19.542)\n",
            "Epoch: [5][130/154]\t Time 2.281 (1.639)\t Data 2.152 (1.507)\t Loss 107.557 (185.279)\t EPE 16.466 (19.469)\n",
            "Epoch: [5][140/154]\t Time 3.625 (1.654)\t Data 3.481 (1.524)\t Loss 119.673 (183.030)\t EPE 16.251 (19.446)\n",
            "Epoch: [5][150/154]\t Time 2.116 (1.670)\t Data 1.994 (1.542)\t Loss 240.265 (182.847)\t EPE 20.887 (19.492)\n",
            "Test: [0/34]\t Time 1.128 (1.128)\t EPE 20.192 (20.192)\n",
            "Test: [10/34]\t Time 0.530 (0.404)\t EPE 15.019 (15.320)\n",
            "Test: [20/34]\t Time 0.190 (0.332)\t EPE 17.615 (15.208)\n",
            "Test: [30/34]\t Time 0.285 (0.290)\t EPE 18.127 (14.268)\n",
            " * EPE 14.351\n",
            "Epoch: [6][0/154]\t Time 3.003 (3.003)\t Data 2.847 (2.847)\t Loss 119.391 (119.391)\t EPE 13.543 (13.543)\n",
            "Epoch: [6][10/154]\t Time 2.486 (1.796)\t Data 2.376 (1.616)\t Loss 187.123 (166.270)\t EPE 22.449 (20.031)\n",
            "Epoch: [6][20/154]\t Time 2.436 (1.730)\t Data 2.308 (1.558)\t Loss 178.153 (171.481)\t EPE 22.433 (20.633)\n",
            "Epoch: [6][30/154]\t Time 2.378 (1.707)\t Data 2.262 (1.535)\t Loss 203.888 (163.708)\t EPE 20.331 (20.324)\n",
            "Epoch: [6][40/154]\t Time 2.544 (1.701)\t Data 2.428 (1.530)\t Loss 189.237 (155.751)\t EPE 17.354 (19.845)\n",
            "Epoch: [6][50/154]\t Time 2.434 (1.697)\t Data 2.322 (1.524)\t Loss 235.627 (156.976)\t EPE 20.948 (19.672)\n",
            "Epoch: [6][60/154]\t Time 2.674 (1.690)\t Data 2.552 (1.516)\t Loss 131.265 (154.346)\t EPE 18.465 (19.586)\n",
            "Epoch: [6][70/154]\t Time 3.705 (1.686)\t Data 3.581 (1.510)\t Loss 131.211 (152.836)\t EPE 18.173 (19.407)\n",
            "Epoch: [6][80/154]\t Time 4.089 (1.680)\t Data 3.948 (1.503)\t Loss 103.024 (154.647)\t EPE 16.493 (19.424)\n",
            "Epoch: [6][90/154]\t Time 4.119 (1.671)\t Data 3.985 (1.493)\t Loss 180.295 (157.840)\t EPE 16.572 (19.456)\n",
            "Epoch: [6][100/154]\t Time 3.999 (1.663)\t Data 3.858 (1.488)\t Loss 180.025 (159.325)\t EPE 16.525 (19.506)\n",
            "Epoch: [6][110/154]\t Time 3.060 (1.657)\t Data 2.910 (1.488)\t Loss 207.556 (161.868)\t EPE 19.160 (19.475)\n",
            "Epoch: [6][120/154]\t Time 2.145 (1.651)\t Data 2.009 (1.488)\t Loss 163.487 (162.244)\t EPE 18.985 (19.409)\n",
            "Epoch: [6][130/154]\t Time 1.976 (1.651)\t Data 1.903 (1.493)\t Loss 160.569 (161.289)\t EPE 22.132 (19.370)\n",
            "Epoch: [6][140/154]\t Time 2.050 (1.683)\t Data 1.929 (1.529)\t Loss 137.823 (159.229)\t EPE 18.611 (19.274)\n",
            "Epoch: [6][150/154]\t Time 2.176 (1.684)\t Data 2.057 (1.532)\t Loss 128.889 (156.950)\t EPE 18.034 (19.174)\n",
            "Test: [0/34]\t Time 0.990 (0.990)\t EPE 20.026 (20.026)\n",
            "Test: [10/34]\t Time 0.484 (0.404)\t EPE 15.667 (15.940)\n",
            "Test: [20/34]\t Time 0.285 (0.343)\t EPE 20.839 (16.041)\n",
            "Test: [30/34]\t Time 0.208 (0.297)\t EPE 18.643 (15.409)\n",
            " * EPE 15.459\n",
            "Epoch: [7][0/154]\t Time 2.973 (2.973)\t Data 2.832 (2.832)\t Loss 176.066 (176.066)\t EPE 24.064 (24.064)\n",
            "Epoch: [7][10/154]\t Time 2.184 (1.750)\t Data 2.068 (1.619)\t Loss 123.406 (125.018)\t EPE 20.123 (19.079)\n",
            "Epoch: [7][20/154]\t Time 2.357 (1.715)\t Data 2.236 (1.599)\t Loss 161.046 (137.412)\t EPE 20.456 (19.937)\n",
            "Epoch: [7][30/154]\t Time 2.053 (1.696)\t Data 1.969 (1.583)\t Loss 137.953 (132.351)\t EPE 22.245 (19.246)\n",
            "Epoch: [7][40/154]\t Time 2.125 (1.688)\t Data 2.003 (1.579)\t Loss 137.073 (135.725)\t EPE 14.990 (19.217)\n",
            "Epoch: [7][50/154]\t Time 2.052 (1.680)\t Data 1.933 (1.572)\t Loss 93.920 (134.681)\t EPE 16.725 (19.064)\n",
            "Epoch: [7][60/154]\t Time 2.331 (1.672)\t Data 2.199 (1.568)\t Loss 143.421 (135.175)\t EPE 20.054 (18.956)\n",
            "Epoch: [7][70/154]\t Time 2.239 (1.664)\t Data 2.118 (1.563)\t Loss 133.630 (135.550)\t EPE 19.513 (19.203)\n",
            "Epoch: [7][80/154]\t Time 2.890 (1.664)\t Data 2.759 (1.566)\t Loss 105.256 (133.284)\t EPE 16.321 (19.032)\n",
            "Epoch: [7][90/154]\t Time 2.281 (1.651)\t Data 2.154 (1.555)\t Loss 133.537 (133.662)\t EPE 23.985 (19.129)\n",
            "Epoch: [7][100/154]\t Time 1.052 (1.641)\t Data 0.991 (1.546)\t Loss 181.504 (135.440)\t EPE 20.674 (19.121)\n",
            "Epoch: [7][110/154]\t Time 1.158 (1.645)\t Data 1.079 (1.550)\t Loss 143.462 (136.475)\t EPE 18.835 (19.119)\n",
            "Epoch: [7][120/154]\t Time 1.685 (1.649)\t Data 1.611 (1.556)\t Loss 184.803 (140.754)\t EPE 16.482 (19.188)\n",
            "Epoch: [7][130/154]\t Time 1.548 (1.683)\t Data 1.482 (1.588)\t Loss 170.611 (140.760)\t EPE 19.252 (19.110)\n",
            "Epoch: [7][140/154]\t Time 1.295 (1.683)\t Data 1.233 (1.589)\t Loss 151.813 (141.256)\t EPE 25.178 (19.067)\n",
            "Epoch: [7][150/154]\t Time 1.052 (1.683)\t Data 0.973 (1.589)\t Loss 121.569 (141.779)\t EPE 17.855 (19.014)\n",
            "Test: [0/34]\t Time 0.670 (0.670)\t EPE 18.135 (18.135)\n",
            "Test: [10/34]\t Time 0.452 (0.261)\t EPE 15.014 (14.773)\n",
            "Test: [20/34]\t Time 0.617 (0.292)\t EPE 15.909 (14.477)\n",
            "Test: [30/34]\t Time 0.475 (0.300)\t EPE 16.876 (13.741)\n",
            " * EPE 13.865\n",
            "Epoch: [8][0/154]\t Time 3.046 (3.046)\t Data 2.907 (2.907)\t Loss 103.770 (103.770)\t EPE 17.115 (17.115)\n",
            "Epoch: [8][10/154]\t Time 2.270 (1.802)\t Data 2.153 (1.663)\t Loss 142.288 (144.790)\t EPE 17.649 (18.243)\n",
            "Epoch: [8][20/154]\t Time 2.087 (1.734)\t Data 1.986 (1.611)\t Loss 106.423 (137.670)\t EPE 16.369 (18.275)\n",
            "Epoch: [8][30/154]\t Time 1.721 (1.704)\t Data 1.646 (1.591)\t Loss 120.531 (134.295)\t EPE 20.811 (18.828)\n",
            "Epoch: [8][40/154]\t Time 1.611 (1.690)\t Data 1.535 (1.583)\t Loss 85.927 (129.556)\t EPE 12.843 (18.344)\n",
            "Epoch: [8][50/154]\t Time 1.471 (1.682)\t Data 1.409 (1.578)\t Loss 139.636 (127.786)\t EPE 16.585 (18.228)\n",
            "Epoch: [8][60/154]\t Time 1.498 (1.680)\t Data 1.423 (1.579)\t Loss 117.923 (128.228)\t EPE 17.763 (18.422)\n",
            "Epoch: [8][70/154]\t Time 2.502 (1.685)\t Data 2.377 (1.586)\t Loss 167.576 (129.043)\t EPE 20.483 (18.515)\n",
            "Epoch: [8][80/154]\t Time 3.727 (1.682)\t Data 3.609 (1.582)\t Loss 121.494 (129.757)\t EPE 17.363 (18.724)\n",
            "Epoch: [8][90/154]\t Time 3.682 (1.667)\t Data 3.566 (1.568)\t Loss 144.596 (129.205)\t EPE 26.143 (18.864)\n",
            "Epoch: [8][100/154]\t Time 3.663 (1.659)\t Data 3.542 (1.561)\t Loss 115.598 (128.012)\t EPE 17.945 (18.848)\n",
            "Epoch: [8][110/154]\t Time 2.593 (1.649)\t Data 2.465 (1.550)\t Loss 97.272 (128.136)\t EPE 16.395 (18.803)\n",
            "Epoch: [8][120/154]\t Time 3.144 (1.694)\t Data 3.018 (1.594)\t Loss 115.341 (127.839)\t EPE 19.942 (18.847)\n",
            "Epoch: [8][130/154]\t Time 2.882 (1.685)\t Data 2.728 (1.586)\t Loss 99.638 (126.721)\t EPE 16.501 (18.776)\n",
            "Epoch: [8][140/154]\t Time 1.748 (1.676)\t Data 1.677 (1.579)\t Loss 102.461 (125.194)\t EPE 17.023 (18.719)\n",
            "Epoch: [8][150/154]\t Time 1.335 (1.673)\t Data 1.273 (1.577)\t Loss 83.929 (124.366)\t EPE 15.233 (18.653)\n",
            "Test: [0/34]\t Time 1.129 (1.129)\t EPE 17.266 (17.266)\n",
            "Test: [10/34]\t Time 0.351 (0.367)\t EPE 14.211 (13.710)\n",
            "Test: [20/34]\t Time 0.239 (0.284)\t EPE 15.918 (13.597)\n",
            "Test: [30/34]\t Time 0.094 (0.254)\t EPE 15.008 (12.782)\n",
            " * EPE 12.874\n",
            "Epoch: [9][0/154]\t Time 2.950 (2.950)\t Data 2.798 (2.798)\t Loss 100.667 (100.667)\t EPE 16.206 (16.206)\n",
            "Epoch: [9][10/154]\t Time 2.424 (1.780)\t Data 2.311 (1.614)\t Loss 105.864 (116.080)\t EPE 19.906 (18.403)\n",
            "Epoch: [9][20/154]\t Time 2.531 (1.731)\t Data 2.413 (1.566)\t Loss 116.855 (112.148)\t EPE 19.419 (18.231)\n",
            "Epoch: [9][30/154]\t Time 2.531 (1.714)\t Data 2.410 (1.543)\t Loss 101.586 (113.336)\t EPE 17.355 (18.298)\n",
            "Epoch: [9][40/154]\t Time 2.546 (1.704)\t Data 2.437 (1.536)\t Loss 91.929 (112.866)\t EPE 16.448 (18.104)\n",
            "Epoch: [9][50/154]\t Time 2.416 (1.704)\t Data 2.295 (1.534)\t Loss 105.163 (113.206)\t EPE 18.048 (18.445)\n",
            "Epoch: [9][60/154]\t Time 2.557 (1.698)\t Data 2.439 (1.527)\t Loss 76.418 (113.452)\t EPE 13.483 (18.562)\n",
            "Epoch: [9][70/154]\t Time 2.732 (1.697)\t Data 2.620 (1.525)\t Loss 99.577 (112.595)\t EPE 16.303 (18.409)\n",
            "Epoch: [9][80/154]\t Time 3.291 (1.695)\t Data 3.181 (1.522)\t Loss 125.868 (113.894)\t EPE 19.436 (18.521)\n",
            "Epoch: [9][90/154]\t Time 4.086 (1.695)\t Data 3.968 (1.522)\t Loss 94.490 (113.931)\t EPE 16.125 (18.600)\n",
            "Epoch: [9][100/154]\t Time 5.031 (1.703)\t Data 4.887 (1.528)\t Loss 157.695 (114.081)\t EPE 22.350 (18.542)\n",
            "Epoch: [9][110/154]\t Time 5.222 (1.745)\t Data 5.067 (1.570)\t Loss 83.940 (113.227)\t EPE 12.609 (18.454)\n",
            "Epoch: [9][120/154]\t Time 4.354 (1.735)\t Data 4.208 (1.559)\t Loss 112.547 (113.130)\t EPE 16.366 (18.412)\n",
            "Epoch: [9][130/154]\t Time 4.139 (1.724)\t Data 4.017 (1.548)\t Loss 110.469 (112.404)\t EPE 20.581 (18.310)\n",
            "Epoch: [9][140/154]\t Time 3.195 (1.715)\t Data 3.041 (1.539)\t Loss 76.964 (112.216)\t EPE 13.235 (18.303)\n",
            "Epoch: [9][150/154]\t Time 2.496 (1.707)\t Data 2.387 (1.531)\t Loss 113.277 (111.972)\t EPE 21.085 (18.293)\n",
            "Test: [0/34]\t Time 1.035 (1.035)\t EPE 16.902 (16.902)\n",
            "Test: [10/34]\t Time 0.351 (0.350)\t EPE 14.425 (13.664)\n",
            "Test: [20/34]\t Time 0.308 (0.278)\t EPE 15.777 (13.522)\n",
            "Test: [30/34]\t Time 0.230 (0.254)\t EPE 15.605 (12.781)\n",
            " * EPE 12.861\n",
            "Epoch: [10][0/154]\t Time 2.938 (2.938)\t Data 2.818 (2.818)\t Loss 76.428 (76.428)\t EPE 13.924 (13.924)\n",
            "Epoch: [10][10/154]\t Time 2.569 (1.806)\t Data 2.458 (1.648)\t Loss 116.552 (101.056)\t EPE 16.072 (17.198)\n",
            "Epoch: [10][20/154]\t Time 2.485 (1.745)\t Data 2.376 (1.579)\t Loss 123.421 (106.487)\t EPE 22.982 (17.709)\n",
            "Epoch: [10][30/154]\t Time 2.510 (1.721)\t Data 2.400 (1.549)\t Loss 138.420 (105.611)\t EPE 25.250 (17.749)\n",
            "Epoch: [10][40/154]\t Time 2.458 (1.705)\t Data 2.343 (1.530)\t Loss 114.087 (106.368)\t EPE 17.670 (17.564)\n",
            "Epoch: [10][50/154]\t Time 2.518 (1.702)\t Data 2.402 (1.526)\t Loss 121.827 (107.761)\t EPE 18.301 (17.806)\n",
            "Epoch: [10][60/154]\t Time 2.505 (1.700)\t Data 2.394 (1.523)\t Loss 154.919 (109.970)\t EPE 27.193 (18.117)\n",
            "Epoch: [10][70/154]\t Time 2.521 (1.697)\t Data 2.405 (1.520)\t Loss 91.725 (109.046)\t EPE 16.655 (17.995)\n",
            "Epoch: [10][80/154]\t Time 2.838 (1.694)\t Data 2.728 (1.517)\t Loss 81.471 (108.880)\t EPE 14.352 (18.039)\n",
            "Epoch: [10][90/154]\t Time 3.609 (1.691)\t Data 3.498 (1.514)\t Loss 109.072 (108.417)\t EPE 20.600 (18.141)\n",
            "Epoch: [10][100/154]\t Time 5.430 (1.704)\t Data 5.273 (1.526)\t Loss 120.439 (107.340)\t EPE 22.498 (17.971)\n",
            "Epoch: [10][110/154]\t Time 4.407 (1.729)\t Data 4.290 (1.550)\t Loss 118.070 (107.408)\t EPE 20.392 (18.010)\n",
            "Epoch: [10][120/154]\t Time 4.581 (1.726)\t Data 4.436 (1.546)\t Loss 87.448 (107.028)\t EPE 14.862 (17.937)\n",
            "Epoch: [10][130/154]\t Time 4.519 (1.718)\t Data 4.401 (1.539)\t Loss 124.082 (107.075)\t EPE 22.879 (17.986)\n",
            "Epoch: [10][140/154]\t Time 3.502 (1.708)\t Data 3.362 (1.534)\t Loss 125.977 (106.897)\t EPE 23.828 (17.982)\n",
            "Epoch: [10][150/154]\t Time 3.117 (1.704)\t Data 2.948 (1.532)\t Loss 100.731 (107.050)\t EPE 18.378 (18.050)\n",
            "Test: [0/34]\t Time 0.915 (0.915)\t EPE 17.159 (17.159)\n",
            "Test: [10/34]\t Time 0.223 (0.307)\t EPE 14.300 (13.466)\n",
            "Test: [20/34]\t Time 0.360 (0.265)\t EPE 16.320 (13.514)\n",
            "Test: [30/34]\t Time 0.098 (0.239)\t EPE 15.277 (12.709)\n",
            " * EPE 12.765\n",
            "Epoch: [11][0/154]\t Time 2.968 (2.968)\t Data 2.830 (2.830)\t Loss 150.822 (150.822)\t EPE 23.209 (23.209)\n",
            "Epoch: [11][10/154]\t Time 2.605 (1.808)\t Data 2.481 (1.649)\t Loss 110.386 (103.937)\t EPE 18.938 (17.694)\n",
            "Epoch: [11][20/154]\t Time 2.463 (1.756)\t Data 2.342 (1.587)\t Loss 84.668 (107.737)\t EPE 14.970 (17.880)\n",
            "Epoch: [11][30/154]\t Time 2.556 (1.737)\t Data 2.438 (1.564)\t Loss 107.813 (108.529)\t EPE 20.440 (18.044)\n",
            "Epoch: [11][40/154]\t Time 2.500 (1.726)\t Data 2.391 (1.551)\t Loss 107.876 (108.185)\t EPE 17.956 (18.184)\n",
            "Epoch: [11][50/154]\t Time 2.623 (1.727)\t Data 2.500 (1.550)\t Loss 129.856 (107.814)\t EPE 19.796 (18.020)\n",
            "Epoch: [11][60/154]\t Time 2.495 (1.725)\t Data 2.373 (1.548)\t Loss 96.573 (107.450)\t EPE 16.466 (17.934)\n",
            "Epoch: [11][70/154]\t Time 2.467 (1.720)\t Data 2.358 (1.541)\t Loss 127.837 (107.593)\t EPE 24.667 (18.081)\n",
            "Epoch: [11][80/154]\t Time 2.596 (1.721)\t Data 2.471 (1.542)\t Loss 93.229 (107.355)\t EPE 16.887 (18.074)\n",
            "Epoch: [11][90/154]\t Time 2.851 (1.725)\t Data 2.706 (1.546)\t Loss 83.481 (105.910)\t EPE 15.532 (17.929)\n",
            "Epoch: [11][100/154]\t Time 2.602 (1.761)\t Data 2.492 (1.583)\t Loss 80.053 (105.960)\t EPE 15.016 (17.954)\n",
            "Epoch: [11][110/154]\t Time 2.561 (1.757)\t Data 2.443 (1.578)\t Loss 124.905 (105.942)\t EPE 24.443 (18.063)\n",
            "Epoch: [11][120/154]\t Time 2.532 (1.755)\t Data 2.409 (1.576)\t Loss 119.561 (106.864)\t EPE 17.440 (18.178)\n",
            "Epoch: [11][130/154]\t Time 2.517 (1.751)\t Data 2.398 (1.571)\t Loss 104.497 (107.539)\t EPE 18.059 (18.233)\n",
            "Epoch: [11][140/154]\t Time 2.472 (1.747)\t Data 2.361 (1.567)\t Loss 81.980 (107.450)\t EPE 13.005 (18.118)\n",
            "Epoch: [11][150/154]\t Time 2.176 (1.742)\t Data 2.063 (1.563)\t Loss 96.648 (107.197)\t EPE 17.487 (18.049)\n",
            "Test: [0/34]\t Time 0.679 (0.679)\t EPE 16.685 (16.685)\n",
            "Test: [10/34]\t Time 0.355 (0.247)\t EPE 14.200 (13.186)\n",
            "Test: [20/34]\t Time 0.297 (0.224)\t EPE 17.052 (13.161)\n",
            "Test: [30/34]\t Time 0.446 (0.233)\t EPE 15.124 (12.483)\n",
            " * EPE 12.517\n",
            "Epoch: [12][0/154]\t Time 4.841 (4.841)\t Data 4.695 (4.695)\t Loss 102.351 (102.351)\t EPE 19.188 (19.188)\n",
            "Epoch: [12][10/154]\t Time 3.963 (1.952)\t Data 3.826 (1.781)\t Loss 89.321 (102.255)\t EPE 15.738 (17.669)\n",
            "Epoch: [12][20/154]\t Time 4.460 (1.809)\t Data 4.310 (1.650)\t Loss 98.383 (104.052)\t EPE 17.634 (17.420)\n",
            "Epoch: [12][30/154]\t Time 4.815 (1.783)\t Data 4.689 (1.619)\t Loss 149.197 (104.788)\t EPE 26.030 (17.834)\n",
            "Epoch: [12][40/154]\t Time 4.097 (1.755)\t Data 3.950 (1.588)\t Loss 107.819 (106.395)\t EPE 19.225 (18.064)\n",
            "Epoch: [12][50/154]\t Time 3.676 (1.733)\t Data 3.503 (1.561)\t Loss 128.267 (105.965)\t EPE 23.342 (17.982)\n",
            "Epoch: [12][60/154]\t Time 2.726 (1.710)\t Data 2.572 (1.545)\t Loss 102.505 (107.885)\t EPE 15.965 (18.046)\n",
            "Epoch: [12][70/154]\t Time 2.191 (1.700)\t Data 2.072 (1.545)\t Loss 93.824 (106.112)\t EPE 17.392 (17.900)\n",
            "Epoch: [12][80/154]\t Time 2.057 (1.699)\t Data 1.934 (1.550)\t Loss 97.770 (105.687)\t EPE 15.957 (17.871)\n",
            "Epoch: [12][90/154]\t Time 2.117 (1.744)\t Data 2.006 (1.600)\t Loss 114.857 (105.379)\t EPE 22.037 (17.830)\n",
            "Epoch: [12][100/154]\t Time 2.080 (1.739)\t Data 1.955 (1.599)\t Loss 144.937 (105.350)\t EPE 23.471 (17.912)\n",
            "Epoch: [12][110/154]\t Time 1.860 (1.731)\t Data 1.784 (1.595)\t Loss 115.933 (106.224)\t EPE 18.284 (18.074)\n",
            "Epoch: [12][120/154]\t Time 2.337 (1.731)\t Data 2.226 (1.598)\t Loss 99.036 (106.797)\t EPE 14.170 (18.195)\n",
            "Epoch: [12][130/154]\t Time 2.578 (1.729)\t Data 2.454 (1.596)\t Loss 83.928 (106.807)\t EPE 13.219 (18.248)\n",
            "Epoch: [12][140/154]\t Time 2.308 (1.726)\t Data 2.192 (1.591)\t Loss 98.698 (106.329)\t EPE 16.701 (18.223)\n",
            "Epoch: [12][150/154]\t Time 1.480 (1.720)\t Data 1.416 (1.587)\t Loss 119.789 (105.772)\t EPE 21.441 (18.162)\n",
            "Test: [0/34]\t Time 0.674 (0.674)\t EPE 16.342 (16.342)\n",
            "Test: [10/34]\t Time 0.364 (0.252)\t EPE 14.490 (13.373)\n",
            "Test: [20/34]\t Time 0.452 (0.236)\t EPE 15.973 (13.289)\n",
            "Test: [30/34]\t Time 0.443 (0.267)\t EPE 15.289 (12.656)\n",
            " * EPE 12.730\n",
            "Epoch: [13][0/154]\t Time 3.756 (3.756)\t Data 3.597 (3.597)\t Loss 92.638 (92.638)\t EPE 14.352 (14.352)\n",
            "Epoch: [13][10/154]\t Time 3.073 (1.851)\t Data 2.949 (1.729)\t Loss 113.531 (100.463)\t EPE 19.169 (17.378)\n",
            "Epoch: [13][20/154]\t Time 3.467 (1.776)\t Data 3.348 (1.665)\t Loss 92.655 (100.418)\t EPE 16.793 (17.854)\n",
            "Epoch: [13][30/154]\t Time 3.955 (1.738)\t Data 3.828 (1.636)\t Loss 92.261 (100.118)\t EPE 17.733 (17.817)\n",
            "Epoch: [13][40/154]\t Time 4.302 (1.715)\t Data 4.185 (1.613)\t Loss 99.875 (100.517)\t EPE 16.628 (17.818)\n",
            "Epoch: [13][50/154]\t Time 4.415 (1.713)\t Data 4.290 (1.599)\t Loss 88.469 (98.677)\t EPE 17.060 (17.630)\n",
            "Epoch: [13][60/154]\t Time 3.883 (1.701)\t Data 3.755 (1.577)\t Loss 91.213 (98.559)\t EPE 17.506 (17.628)\n",
            "Epoch: [13][70/154]\t Time 2.832 (1.686)\t Data 2.715 (1.555)\t Loss 101.142 (99.280)\t EPE 17.207 (17.693)\n",
            "Epoch: [13][80/154]\t Time 2.271 (1.680)\t Data 2.138 (1.552)\t Loss 87.071 (101.183)\t EPE 15.700 (18.126)\n",
            "Epoch: [13][90/154]\t Time 2.350 (1.727)\t Data 2.227 (1.601)\t Loss 126.020 (102.098)\t EPE 23.641 (18.238)\n",
            "Epoch: [13][100/154]\t Time 1.858 (1.720)\t Data 1.789 (1.598)\t Loss 109.256 (102.466)\t EPE 17.483 (18.277)\n",
            "Epoch: [13][110/154]\t Time 1.822 (1.716)\t Data 1.745 (1.597)\t Loss 131.402 (102.564)\t EPE 23.476 (18.222)\n",
            "Epoch: [13][120/154]\t Time 1.493 (1.711)\t Data 1.417 (1.595)\t Loss 100.697 (103.104)\t EPE 15.422 (18.327)\n",
            "Epoch: [13][130/154]\t Time 0.993 (1.707)\t Data 0.932 (1.593)\t Loss 157.197 (102.638)\t EPE 25.878 (18.222)\n",
            "Epoch: [13][140/154]\t Time 0.649 (1.706)\t Data 0.577 (1.592)\t Loss 77.721 (102.524)\t EPE 14.492 (18.187)\n",
            "Epoch: [13][150/154]\t Time 0.910 (1.705)\t Data 0.827 (1.593)\t Loss 103.034 (102.623)\t EPE 20.263 (18.192)\n",
            "Test: [0/34]\t Time 0.668 (0.668)\t EPE 16.245 (16.245)\n",
            "Test: [10/34]\t Time 0.326 (0.252)\t EPE 13.905 (12.882)\n",
            "Test: [20/34]\t Time 0.303 (0.228)\t EPE 15.940 (12.836)\n",
            "Test: [30/34]\t Time 0.647 (0.236)\t EPE 14.470 (12.064)\n",
            " * EPE 12.118\n",
            "Epoch: [14][0/154]\t Time 4.575 (4.575)\t Data 4.419 (4.419)\t Loss 76.338 (76.338)\t EPE 13.764 (13.764)\n",
            "Epoch: [14][10/154]\t Time 4.650 (1.995)\t Data 4.521 (1.836)\t Loss 77.783 (103.935)\t EPE 14.891 (17.413)\n",
            "Epoch: [14][20/154]\t Time 4.385 (1.825)\t Data 4.251 (1.654)\t Loss 121.062 (101.647)\t EPE 17.832 (16.976)\n",
            "Epoch: [14][30/154]\t Time 4.037 (1.752)\t Data 3.894 (1.604)\t Loss 105.107 (99.717)\t EPE 18.210 (17.071)\n",
            "Epoch: [14][40/154]\t Time 3.536 (1.722)\t Data 3.374 (1.585)\t Loss 132.159 (99.934)\t EPE 23.320 (17.315)\n",
            "Epoch: [14][50/154]\t Time 2.490 (1.697)\t Data 2.363 (1.566)\t Loss 96.701 (100.004)\t EPE 18.305 (17.455)\n",
            "Epoch: [14][60/154]\t Time 2.456 (1.694)\t Data 2.340 (1.568)\t Loss 139.662 (100.697)\t EPE 22.611 (17.636)\n",
            "Epoch: [14][70/154]\t Time 1.952 (1.691)\t Data 1.872 (1.569)\t Loss 102.599 (101.275)\t EPE 17.241 (17.659)\n",
            "Epoch: [14][80/154]\t Time 1.816 (1.742)\t Data 1.754 (1.622)\t Loss 122.931 (102.244)\t EPE 23.145 (17.853)\n",
            "Epoch: [14][90/154]\t Time 2.041 (1.737)\t Data 1.924 (1.620)\t Loss 71.631 (102.465)\t EPE 13.021 (17.856)\n",
            "Epoch: [14][100/154]\t Time 2.195 (1.733)\t Data 2.085 (1.618)\t Loss 95.376 (102.974)\t EPE 17.419 (17.967)\n",
            "Epoch: [14][110/154]\t Time 1.786 (1.727)\t Data 1.716 (1.614)\t Loss 116.451 (104.200)\t EPE 21.501 (18.156)\n",
            "Epoch: [14][120/154]\t Time 1.733 (1.724)\t Data 1.656 (1.613)\t Loss 117.751 (103.935)\t EPE 20.422 (18.088)\n",
            "Epoch: [14][130/154]\t Time 1.881 (1.721)\t Data 1.795 (1.612)\t Loss 76.637 (103.295)\t EPE 13.464 (17.982)\n",
            "Epoch: [14][140/154]\t Time 2.557 (1.719)\t Data 2.443 (1.611)\t Loss 116.997 (103.652)\t EPE 21.613 (17.987)\n",
            "Epoch: [14][150/154]\t Time 3.480 (1.718)\t Data 3.360 (1.610)\t Loss 88.906 (103.346)\t EPE 17.371 (17.932)\n",
            "Test: [0/34]\t Time 0.662 (0.662)\t EPE 17.056 (17.056)\n",
            "Test: [10/34]\t Time 0.372 (0.245)\t EPE 14.374 (13.360)\n",
            "Test: [20/34]\t Time 0.246 (0.221)\t EPE 17.667 (13.628)\n",
            "Test: [30/34]\t Time 0.228 (0.215)\t EPE 15.266 (12.921)\n",
            " * EPE 12.938\n",
            "Epoch: [15][0/154]\t Time 5.267 (5.267)\t Data 5.094 (5.094)\t Loss 71.880 (71.880)\t EPE 13.643 (13.643)\n",
            "Epoch: [15][10/154]\t Time 4.413 (1.971)\t Data 4.295 (1.797)\t Loss 109.175 (101.013)\t EPE 19.831 (18.405)\n",
            "Epoch: [15][20/154]\t Time 4.263 (1.831)\t Data 4.115 (1.659)\t Loss 101.252 (101.736)\t EPE 18.003 (18.008)\n",
            "Epoch: [15][30/154]\t Time 3.709 (1.779)\t Data 3.575 (1.607)\t Loss 124.997 (104.125)\t EPE 20.325 (18.303)\n",
            "Epoch: [15][40/154]\t Time 2.917 (1.747)\t Data 2.760 (1.572)\t Loss 106.573 (102.854)\t EPE 19.088 (18.171)\n",
            "Epoch: [15][50/154]\t Time 2.453 (1.732)\t Data 2.337 (1.556)\t Loss 105.178 (101.053)\t EPE 19.552 (17.853)\n",
            "Epoch: [15][60/154]\t Time 2.506 (1.733)\t Data 2.396 (1.558)\t Loss 102.062 (100.184)\t EPE 18.681 (17.682)\n",
            "Epoch: [15][70/154]\t Time 2.679 (1.794)\t Data 2.555 (1.617)\t Loss 109.591 (100.602)\t EPE 16.883 (17.782)\n",
            "Epoch: [15][80/154]\t Time 2.494 (1.781)\t Data 2.384 (1.604)\t Loss 104.228 (101.029)\t EPE 19.623 (17.814)\n",
            "Epoch: [15][90/154]\t Time 2.578 (1.776)\t Data 2.462 (1.599)\t Loss 92.713 (100.749)\t EPE 15.927 (17.838)\n",
            "Epoch: [15][100/154]\t Time 2.556 (1.769)\t Data 2.441 (1.592)\t Loss 94.615 (100.879)\t EPE 17.668 (17.931)\n",
            "Epoch: [15][110/154]\t Time 2.476 (1.762)\t Data 2.352 (1.584)\t Loss 81.352 (100.999)\t EPE 13.631 (17.880)\n",
            "Epoch: [15][120/154]\t Time 2.650 (1.759)\t Data 2.539 (1.580)\t Loss 90.664 (101.186)\t EPE 14.903 (17.856)\n",
            "Epoch: [15][130/154]\t Time 2.607 (1.756)\t Data 2.494 (1.578)\t Loss 93.345 (101.153)\t EPE 18.030 (17.836)\n",
            "Epoch: [15][140/154]\t Time 2.529 (1.751)\t Data 2.403 (1.573)\t Loss 130.413 (101.568)\t EPE 24.148 (17.941)\n",
            "Epoch: [15][150/154]\t Time 2.619 (1.747)\t Data 2.490 (1.568)\t Loss 83.477 (101.319)\t EPE 15.052 (17.956)\n",
            "Test: [0/34]\t Time 0.665 (0.665)\t EPE 16.785 (16.785)\n",
            "Test: [10/34]\t Time 0.224 (0.230)\t EPE 14.353 (13.145)\n",
            "Test: [20/34]\t Time 0.521 (0.260)\t EPE 17.329 (13.385)\n",
            "Test: [30/34]\t Time 0.474 (0.278)\t EPE 15.464 (12.651)\n",
            " * EPE 12.670\n",
            "Epoch: [16][0/154]\t Time 3.285 (3.285)\t Data 3.148 (3.148)\t Loss 97.816 (97.816)\t EPE 17.355 (17.355)\n",
            "Epoch: [16][10/154]\t Time 2.992 (1.806)\t Data 2.877 (1.632)\t Loss 98.863 (103.653)\t EPE 16.462 (18.478)\n",
            "Epoch: [16][20/154]\t Time 3.572 (1.767)\t Data 3.450 (1.591)\t Loss 79.966 (106.907)\t EPE 15.487 (18.741)\n",
            "Epoch: [16][30/154]\t Time 4.269 (1.734)\t Data 4.153 (1.556)\t Loss 96.304 (104.706)\t EPE 16.239 (18.477)\n",
            "Epoch: [16][40/154]\t Time 4.673 (1.713)\t Data 4.547 (1.535)\t Loss 92.734 (100.569)\t EPE 17.717 (17.937)\n",
            "Epoch: [16][50/154]\t Time 4.556 (1.696)\t Data 4.429 (1.516)\t Loss 95.264 (101.045)\t EPE 18.687 (18.260)\n",
            "Epoch: [16][60/154]\t Time 3.181 (1.779)\t Data 3.054 (1.597)\t Loss 90.006 (100.178)\t EPE 15.901 (18.173)\n",
            "Epoch: [16][70/154]\t Time 4.025 (1.768)\t Data 3.901 (1.585)\t Loss 89.829 (100.599)\t EPE 15.351 (18.270)\n",
            "Epoch: [16][80/154]\t Time 4.217 (1.758)\t Data 4.090 (1.575)\t Loss 144.598 (100.986)\t EPE 27.581 (18.390)\n",
            "Epoch: [16][90/154]\t Time 4.533 (1.744)\t Data 4.395 (1.562)\t Loss 90.918 (99.555)\t EPE 16.247 (18.142)\n",
            "Epoch: [16][100/154]\t Time 4.900 (1.738)\t Data 4.778 (1.556)\t Loss 79.005 (98.541)\t EPE 14.321 (17.929)\n",
            "Epoch: [16][110/154]\t Time 4.562 (1.732)\t Data 4.450 (1.550)\t Loss 93.607 (98.516)\t EPE 15.339 (17.972)\n",
            "Epoch: [16][120/154]\t Time 3.982 (1.727)\t Data 3.862 (1.546)\t Loss 79.909 (98.592)\t EPE 14.332 (17.951)\n",
            "Epoch: [16][130/154]\t Time 3.115 (1.718)\t Data 2.945 (1.536)\t Loss 99.621 (98.880)\t EPE 17.422 (17.991)\n",
            "Epoch: [16][140/154]\t Time 2.493 (1.711)\t Data 2.368 (1.530)\t Loss 103.513 (99.073)\t EPE 19.991 (17.980)\n",
            "Epoch: [16][150/154]\t Time 2.425 (1.710)\t Data 2.317 (1.530)\t Loss 67.910 (99.108)\t EPE 11.797 (17.937)\n",
            "Test: [0/34]\t Time 1.113 (1.113)\t EPE 15.998 (15.998)\n",
            "Test: [10/34]\t Time 0.453 (0.380)\t EPE 14.202 (12.913)\n",
            "Test: [20/34]\t Time 0.186 (0.305)\t EPE 15.896 (12.864)\n",
            "Test: [30/34]\t Time 0.114 (0.271)\t EPE 14.826 (12.191)\n",
            " * EPE 12.233\n",
            "Epoch: [17][0/154]\t Time 2.937 (2.937)\t Data 2.788 (2.788)\t Loss 91.990 (91.990)\t EPE 17.551 (17.551)\n",
            "Epoch: [17][10/154]\t Time 2.038 (1.746)\t Data 1.928 (1.624)\t Loss 119.050 (95.854)\t EPE 22.185 (17.520)\n",
            "Epoch: [17][20/154]\t Time 2.383 (1.709)\t Data 2.273 (1.599)\t Loss 90.474 (97.622)\t EPE 17.695 (17.959)\n",
            "Epoch: [17][30/154]\t Time 2.494 (1.702)\t Data 2.374 (1.586)\t Loss 109.608 (98.927)\t EPE 19.920 (17.857)\n",
            "Epoch: [17][40/154]\t Time 2.537 (1.693)\t Data 2.421 (1.563)\t Loss 95.563 (98.290)\t EPE 17.990 (17.725)\n",
            "Epoch: [17][50/154]\t Time 3.837 (1.768)\t Data 3.719 (1.625)\t Loss 79.496 (98.511)\t EPE 14.825 (17.831)\n",
            "Epoch: [17][60/154]\t Time 4.113 (1.751)\t Data 4.004 (1.603)\t Loss 70.590 (98.932)\t EPE 11.807 (17.879)\n",
            "Epoch: [17][70/154]\t Time 4.668 (1.738)\t Data 4.525 (1.585)\t Loss 143.790 (99.783)\t EPE 26.693 (17.941)\n",
            "Epoch: [17][80/154]\t Time 4.289 (1.722)\t Data 4.174 (1.566)\t Loss 75.852 (100.295)\t EPE 14.255 (18.002)\n",
            "Epoch: [17][90/154]\t Time 3.771 (1.701)\t Data 3.620 (1.542)\t Loss 75.681 (99.716)\t EPE 14.159 (17.939)\n",
            "Epoch: [17][100/154]\t Time 2.867 (1.686)\t Data 2.741 (1.526)\t Loss 102.338 (100.170)\t EPE 18.004 (18.094)\n",
            "Epoch: [17][110/154]\t Time 2.348 (1.676)\t Data 2.226 (1.513)\t Loss 81.837 (100.009)\t EPE 16.045 (18.119)\n",
            "Epoch: [17][120/154]\t Time 2.407 (1.675)\t Data 2.290 (1.511)\t Loss 86.209 (99.739)\t EPE 13.960 (18.096)\n",
            "Epoch: [17][130/154]\t Time 2.400 (1.672)\t Data 2.290 (1.507)\t Loss 108.586 (99.206)\t EPE 20.286 (17.989)\n",
            "Epoch: [17][140/154]\t Time 2.456 (1.670)\t Data 2.333 (1.504)\t Loss 98.759 (99.352)\t EPE 18.767 (17.989)\n",
            "Epoch: [17][150/154]\t Time 2.355 (1.667)\t Data 2.246 (1.504)\t Loss 74.895 (98.501)\t EPE 14.126 (17.841)\n",
            "Test: [0/34]\t Time 0.647 (0.647)\t EPE 16.263 (16.263)\n",
            "Test: [10/34]\t Time 0.331 (0.233)\t EPE 13.821 (12.685)\n",
            "Test: [20/34]\t Time 0.445 (0.276)\t EPE 16.506 (12.742)\n",
            "Test: [30/34]\t Time 0.538 (0.291)\t EPE 14.370 (12.027)\n",
            " * EPE 12.060\n",
            "Epoch: [18][0/154]\t Time 2.974 (2.974)\t Data 2.828 (2.828)\t Loss 80.711 (80.711)\t EPE 14.746 (14.746)\n",
            "Epoch: [18][10/154]\t Time 2.498 (1.796)\t Data 2.381 (1.619)\t Loss 91.949 (93.374)\t EPE 16.830 (16.865)\n",
            "Epoch: [18][20/154]\t Time 3.045 (1.738)\t Data 2.926 (1.561)\t Loss 79.718 (94.008)\t EPE 13.703 (17.121)\n",
            "Epoch: [18][30/154]\t Time 3.546 (1.704)\t Data 3.430 (1.527)\t Loss 90.932 (96.461)\t EPE 17.576 (17.610)\n",
            "Epoch: [18][40/154]\t Time 4.202 (1.680)\t Data 4.068 (1.501)\t Loss 80.322 (97.260)\t EPE 13.587 (17.737)\n",
            "Epoch: [18][50/154]\t Time 4.910 (1.754)\t Data 4.778 (1.575)\t Loss 108.037 (97.889)\t EPE 19.545 (17.896)\n",
            "Epoch: [18][60/154]\t Time 4.521 (1.728)\t Data 4.374 (1.549)\t Loss 133.409 (97.353)\t EPE 24.933 (17.813)\n",
            "Epoch: [18][70/154]\t Time 3.311 (1.704)\t Data 3.188 (1.526)\t Loss 156.090 (99.328)\t EPE 16.012 (17.921)\n",
            "Epoch: [18][80/154]\t Time 2.713 (1.688)\t Data 2.574 (1.510)\t Loss 234.981 (113.718)\t EPE 13.993 (18.125)\n",
            "Epoch: [18][90/154]\t Time 2.420 (1.686)\t Data 2.306 (1.508)\t Loss 216.750 (126.144)\t EPE 20.081 (18.274)\n",
            "Epoch: [18][100/154]\t Time 2.537 (1.687)\t Data 2.419 (1.509)\t Loss 118.022 (129.120)\t EPE 15.351 (18.225)\n",
            "Epoch: [18][110/154]\t Time 2.351 (1.684)\t Data 2.231 (1.506)\t Loss 134.765 (130.298)\t EPE 19.774 (18.219)\n",
            "Epoch: [18][120/154]\t Time 2.510 (1.681)\t Data 2.393 (1.503)\t Loss 148.430 (132.395)\t EPE 16.588 (18.143)\n",
            "Epoch: [18][130/154]\t Time 2.352 (1.678)\t Data 2.230 (1.499)\t Loss 125.957 (133.473)\t EPE 18.755 (18.234)\n",
            "Epoch: [18][140/154]\t Time 2.465 (1.676)\t Data 2.349 (1.497)\t Loss 93.877 (132.189)\t EPE 14.446 (18.150)\n",
            "Epoch: [18][150/154]\t Time 1.980 (1.674)\t Data 1.914 (1.495)\t Loss 102.375 (130.376)\t EPE 16.103 (18.102)\n",
            "Test: [0/34]\t Time 0.715 (0.715)\t EPE 16.330 (16.330)\n",
            "Test: [10/34]\t Time 0.081 (0.223)\t EPE 14.528 (13.169)\n",
            "Test: [20/34]\t Time 0.096 (0.211)\t EPE 17.289 (13.229)\n",
            "Test: [30/34]\t Time 0.102 (0.226)\t EPE 15.544 (12.671)\n",
            " * EPE 12.713\n",
            "Epoch: [19][0/154]\t Time 4.641 (4.641)\t Data 4.493 (4.493)\t Loss 83.133 (83.133)\t EPE 12.658 (12.658)\n",
            "Epoch: [19][10/154]\t Time 4.325 (1.922)\t Data 4.191 (1.747)\t Loss 106.129 (100.461)\t EPE 19.578 (17.619)\n",
            "Epoch: [19][20/154]\t Time 4.371 (1.758)\t Data 4.239 (1.584)\t Loss 124.821 (101.308)\t EPE 21.595 (17.796)\n",
            "Epoch: [19][30/154]\t Time 3.250 (1.684)\t Data 3.132 (1.511)\t Loss 79.971 (97.786)\t EPE 15.020 (17.042)\n",
            "Epoch: [19][40/154]\t Time 1.962 (1.647)\t Data 1.865 (1.493)\t Loss 105.442 (100.162)\t EPE 19.486 (17.754)\n",
            "Epoch: [19][50/154]\t Time 2.004 (1.646)\t Data 1.910 (1.505)\t Loss 96.651 (99.018)\t EPE 17.712 (17.675)\n",
            "Epoch: [19][60/154]\t Time 1.676 (1.706)\t Data 1.616 (1.572)\t Loss 85.728 (97.252)\t EPE 16.384 (17.458)\n",
            "Epoch: [19][70/154]\t Time 1.716 (1.696)\t Data 1.648 (1.569)\t Loss 124.194 (96.896)\t EPE 22.009 (17.446)\n",
            "Epoch: [19][80/154]\t Time 1.702 (1.687)\t Data 1.635 (1.565)\t Loss 88.114 (96.704)\t EPE 14.065 (17.382)\n",
            "Epoch: [19][90/154]\t Time 2.322 (1.683)\t Data 2.189 (1.565)\t Loss 80.771 (96.660)\t EPE 14.353 (17.449)\n",
            "Epoch: [19][100/154]\t Time 2.960 (1.679)\t Data 2.837 (1.560)\t Loss 93.120 (96.197)\t EPE 17.079 (17.431)\n",
            "Epoch: [19][110/154]\t Time 3.438 (1.678)\t Data 3.328 (1.554)\t Loss 99.285 (97.075)\t EPE 19.096 (17.677)\n",
            "Epoch: [19][120/154]\t Time 3.874 (1.671)\t Data 3.739 (1.542)\t Loss 118.798 (96.332)\t EPE 21.876 (17.579)\n",
            "Epoch: [19][130/154]\t Time 4.132 (1.664)\t Data 4.001 (1.536)\t Loss 106.154 (97.056)\t EPE 21.446 (17.717)\n",
            "Epoch: [19][140/154]\t Time 3.514 (1.658)\t Data 3.392 (1.533)\t Loss 96.797 (96.821)\t EPE 18.408 (17.701)\n",
            "Epoch: [19][150/154]\t Time 3.030 (1.654)\t Data 2.870 (1.531)\t Loss 95.728 (96.566)\t EPE 18.625 (17.678)\n",
            "Test: [0/34]\t Time 0.729 (0.729)\t EPE 15.943 (15.943)\n",
            "Test: [10/34]\t Time 0.227 (0.265)\t EPE 13.997 (12.702)\n",
            "Test: [20/34]\t Time 0.313 (0.235)\t EPE 15.772 (12.713)\n",
            "Test: [30/34]\t Time 0.236 (0.224)\t EPE 14.444 (11.990)\n",
            " * EPE 12.031\n",
            "Epoch: [20][0/154]\t Time 4.675 (4.675)\t Data 4.515 (4.515)\t Loss 78.589 (78.589)\t EPE 14.372 (14.372)\n",
            "Epoch: [20][10/154]\t Time 3.932 (1.850)\t Data 3.789 (1.741)\t Loss 79.285 (95.107)\t EPE 15.082 (17.964)\n",
            "Epoch: [20][20/154]\t Time 3.333 (1.719)\t Data 3.219 (1.616)\t Loss 123.134 (98.302)\t EPE 20.971 (18.180)\n",
            "Epoch: [20][30/154]\t Time 2.476 (1.683)\t Data 2.351 (1.588)\t Loss 99.024 (95.132)\t EPE 18.046 (17.682)\n",
            "Epoch: [20][40/154]\t Time 1.782 (1.661)\t Data 1.717 (1.569)\t Loss 84.104 (93.150)\t EPE 14.999 (17.395)\n",
            "Epoch: [20][50/154]\t Time 1.678 (1.659)\t Data 1.613 (1.569)\t Loss 80.789 (91.648)\t EPE 15.649 (17.160)\n",
            "Epoch: [20][60/154]\t Time 1.732 (1.730)\t Data 1.668 (1.637)\t Loss 100.503 (91.147)\t EPE 19.813 (17.112)\n",
            "Epoch: [20][70/154]\t Time 1.340 (1.721)\t Data 1.274 (1.628)\t Loss 89.739 (91.739)\t EPE 17.746 (17.287)\n",
            "Epoch: [20][80/154]\t Time 1.371 (1.716)\t Data 1.300 (1.624)\t Loss 79.125 (92.247)\t EPE 15.106 (17.379)\n",
            "Epoch: [20][90/154]\t Time 1.568 (1.713)\t Data 1.503 (1.621)\t Loss 111.560 (92.458)\t EPE 21.882 (17.449)\n",
            "Epoch: [20][100/154]\t Time 1.071 (1.707)\t Data 0.992 (1.615)\t Loss 117.107 (93.345)\t EPE 21.709 (17.588)\n",
            "Epoch: [20][110/154]\t Time 0.951 (1.705)\t Data 0.877 (1.614)\t Loss 94.359 (93.614)\t EPE 16.296 (17.589)\n",
            "Epoch: [20][120/154]\t Time 0.722 (1.701)\t Data 0.646 (1.610)\t Loss 86.071 (93.834)\t EPE 16.153 (17.631)\n",
            "Epoch: [20][130/154]\t Time 0.666 (1.698)\t Data 0.603 (1.607)\t Loss 98.165 (93.334)\t EPE 18.828 (17.542)\n",
            "Epoch: [20][140/154]\t Time 0.699 (1.689)\t Data 0.619 (1.597)\t Loss 86.781 (93.411)\t EPE 16.621 (17.572)\n",
            "Epoch: [20][150/154]\t Time 1.454 (1.686)\t Data 1.371 (1.594)\t Loss 86.946 (93.785)\t EPE 16.300 (17.648)\n",
            "Test: [0/34]\t Time 0.669 (0.669)\t EPE 16.310 (16.310)\n",
            "Test: [10/34]\t Time 0.205 (0.227)\t EPE 14.023 (12.696)\n",
            "Test: [20/34]\t Time 0.297 (0.216)\t EPE 17.345 (12.869)\n",
            "Test: [30/34]\t Time 0.379 (0.213)\t EPE 14.781 (12.161)\n",
            " * EPE 12.169\n",
            "Epoch: [21][0/154]\t Time 4.441 (4.441)\t Data 4.262 (4.262)\t Loss 78.589 (78.589)\t EPE 14.490 (14.490)\n",
            "Epoch: [21][10/154]\t Time 3.293 (1.915)\t Data 3.164 (1.736)\t Loss 94.094 (95.961)\t EPE 18.374 (18.019)\n",
            "Epoch: [21][20/154]\t Time 2.614 (1.766)\t Data 2.489 (1.586)\t Loss 89.438 (95.156)\t EPE 17.895 (17.871)\n",
            "Epoch: [21][30/154]\t Time 2.430 (1.724)\t Data 2.314 (1.545)\t Loss 85.831 (96.100)\t EPE 16.221 (17.994)\n",
            "Epoch: [21][40/154]\t Time 2.350 (1.706)\t Data 2.232 (1.539)\t Loss 85.051 (95.723)\t EPE 15.774 (18.022)\n",
            "Epoch: [21][50/154]\t Time 2.526 (1.703)\t Data 2.402 (1.542)\t Loss 81.774 (95.525)\t EPE 16.024 (17.986)\n",
            "Epoch: [21][60/154]\t Time 2.475 (1.761)\t Data 2.356 (1.597)\t Loss 104.070 (94.559)\t EPE 20.453 (17.832)\n",
            "Epoch: [21][70/154]\t Time 2.480 (1.750)\t Data 2.355 (1.586)\t Loss 87.012 (94.940)\t EPE 17.159 (17.929)\n",
            "Epoch: [21][80/154]\t Time 2.209 (1.739)\t Data 2.080 (1.577)\t Loss 86.411 (94.987)\t EPE 17.494 (17.966)\n",
            "Epoch: [21][90/154]\t Time 1.686 (1.725)\t Data 1.607 (1.570)\t Loss 77.463 (95.018)\t EPE 15.248 (17.938)\n",
            "Epoch: [21][100/154]\t Time 2.698 (1.718)\t Data 2.574 (1.570)\t Loss 82.787 (94.739)\t EPE 15.669 (17.913)\n",
            "Epoch: [21][110/154]\t Time 2.964 (1.707)\t Data 2.834 (1.564)\t Loss 83.122 (94.384)\t EPE 15.285 (17.875)\n",
            "Epoch: [21][120/154]\t Time 3.199 (1.697)\t Data 3.053 (1.560)\t Loss 103.953 (94.409)\t EPE 18.172 (17.878)\n",
            "Epoch: [21][130/154]\t Time 2.640 (1.684)\t Data 2.510 (1.551)\t Loss 89.923 (94.467)\t EPE 16.268 (17.859)\n",
            "Epoch: [21][140/154]\t Time 1.792 (1.675)\t Data 1.718 (1.546)\t Loss 107.734 (94.257)\t EPE 17.070 (17.812)\n",
            "Epoch: [21][150/154]\t Time 1.567 (1.673)\t Data 1.495 (1.547)\t Loss 114.816 (94.329)\t EPE 21.612 (17.834)\n",
            "Test: [0/34]\t Time 1.063 (1.063)\t EPE 15.953 (15.953)\n",
            "Test: [10/34]\t Time 0.438 (0.394)\t EPE 13.918 (12.551)\n",
            "Test: [20/34]\t Time 0.243 (0.297)\t EPE 16.344 (12.583)\n",
            "Test: [30/34]\t Time 0.150 (0.263)\t EPE 14.371 (11.883)\n",
            " * EPE 11.904\n",
            "Epoch: [22][0/154]\t Time 2.947 (2.947)\t Data 2.790 (2.790)\t Loss 77.086 (77.086)\t EPE 14.350 (14.350)\n",
            "Epoch: [22][10/154]\t Time 2.420 (1.779)\t Data 2.304 (1.600)\t Loss 67.222 (90.811)\t EPE 11.274 (17.060)\n",
            "Epoch: [22][20/154]\t Time 2.533 (1.744)\t Data 2.413 (1.562)\t Loss 77.127 (93.813)\t EPE 15.169 (17.686)\n",
            "Epoch: [22][30/154]\t Time 2.474 (1.716)\t Data 2.354 (1.536)\t Loss 98.337 (93.656)\t EPE 18.185 (17.599)\n",
            "Epoch: [22][40/154]\t Time 2.508 (1.705)\t Data 2.377 (1.527)\t Loss 94.492 (93.545)\t EPE 16.961 (17.562)\n",
            "Epoch: [22][50/154]\t Time 2.480 (1.704)\t Data 2.358 (1.526)\t Loss 125.888 (92.993)\t EPE 24.963 (17.547)\n",
            "Epoch: [22][60/154]\t Time 2.478 (1.772)\t Data 2.356 (1.593)\t Loss 100.030 (93.839)\t EPE 15.630 (17.685)\n",
            "Epoch: [22][70/154]\t Time 2.587 (1.760)\t Data 2.466 (1.580)\t Loss 96.549 (93.544)\t EPE 18.672 (17.603)\n",
            "Epoch: [22][80/154]\t Time 2.958 (1.744)\t Data 2.848 (1.566)\t Loss 91.919 (93.450)\t EPE 17.774 (17.613)\n",
            "Epoch: [22][90/154]\t Time 3.224 (1.735)\t Data 3.114 (1.566)\t Loss 85.145 (93.781)\t EPE 16.001 (17.684)\n",
            "Epoch: [22][100/154]\t Time 3.931 (1.725)\t Data 3.817 (1.562)\t Loss 97.738 (94.615)\t EPE 18.822 (17.821)\n",
            "Epoch: [22][110/154]\t Time 4.320 (1.716)\t Data 4.190 (1.553)\t Loss 108.211 (94.948)\t EPE 19.818 (17.901)\n",
            "Epoch: [22][120/154]\t Time 3.728 (1.704)\t Data 3.557 (1.546)\t Loss 84.366 (94.397)\t EPE 15.722 (17.813)\n",
            "Epoch: [22][130/154]\t Time 2.524 (1.693)\t Data 2.395 (1.539)\t Loss 98.361 (94.062)\t EPE 19.407 (17.777)\n",
            "Epoch: [22][140/154]\t Time 2.100 (1.689)\t Data 1.981 (1.539)\t Loss 81.973 (93.677)\t EPE 15.593 (17.729)\n",
            "Epoch: [22][150/154]\t Time 2.090 (1.686)\t Data 1.975 (1.540)\t Loss 75.687 (93.303)\t EPE 13.981 (17.630)\n",
            "Test: [0/34]\t Time 0.952 (0.952)\t EPE 15.889 (15.889)\n",
            "Test: [10/34]\t Time 0.447 (0.386)\t EPE 13.967 (12.571)\n",
            "Test: [20/34]\t Time 0.110 (0.324)\t EPE 16.551 (12.631)\n",
            "Test: [30/34]\t Time 0.236 (0.283)\t EPE 14.681 (11.947)\n",
            " * EPE 11.971\n",
            "Epoch: [23][0/154]\t Time 2.936 (2.936)\t Data 2.800 (2.800)\t Loss 103.197 (103.197)\t EPE 20.311 (20.311)\n",
            "Epoch: [23][10/154]\t Time 2.478 (1.819)\t Data 2.360 (1.659)\t Loss 75.690 (94.169)\t EPE 14.268 (17.540)\n",
            "Epoch: [23][20/154]\t Time 2.471 (1.756)\t Data 2.361 (1.586)\t Loss 97.552 (93.586)\t EPE 16.174 (17.466)\n",
            "Epoch: [23][30/154]\t Time 2.536 (1.727)\t Data 2.419 (1.552)\t Loss 114.838 (94.988)\t EPE 21.333 (17.762)\n",
            "Epoch: [23][40/154]\t Time 2.565 (1.718)\t Data 2.444 (1.543)\t Loss 84.189 (93.604)\t EPE 16.133 (17.552)\n",
            "Epoch: [23][50/154]\t Time 2.686 (1.713)\t Data 2.563 (1.537)\t Loss 82.960 (94.111)\t EPE 15.091 (17.690)\n",
            "Epoch: [23][60/154]\t Time 2.616 (1.785)\t Data 2.498 (1.607)\t Loss 85.092 (94.155)\t EPE 15.736 (17.726)\n",
            "Epoch: [23][70/154]\t Time 2.863 (1.777)\t Data 2.751 (1.598)\t Loss 68.307 (94.084)\t EPE 12.953 (17.748)\n",
            "Epoch: [23][80/154]\t Time 2.768 (1.764)\t Data 2.647 (1.587)\t Loss 95.478 (94.170)\t EPE 17.777 (17.750)\n",
            "Epoch: [23][90/154]\t Time 3.163 (1.752)\t Data 3.044 (1.584)\t Loss 85.316 (94.445)\t EPE 16.008 (17.801)\n",
            "Epoch: [23][100/154]\t Time 3.103 (1.735)\t Data 2.968 (1.575)\t Loss 85.857 (94.143)\t EPE 16.299 (17.732)\n",
            "Epoch: [23][110/154]\t Time 2.919 (1.721)\t Data 2.791 (1.569)\t Loss 96.633 (94.686)\t EPE 19.363 (17.802)\n",
            "Epoch: [23][120/154]\t Time 1.714 (1.704)\t Data 1.637 (1.559)\t Loss 95.255 (94.318)\t EPE 18.496 (17.729)\n",
            "Epoch: [23][130/154]\t Time 1.362 (1.699)\t Data 1.290 (1.558)\t Loss 74.814 (93.816)\t EPE 14.002 (17.643)\n",
            "Epoch: [23][140/154]\t Time 0.996 (1.695)\t Data 0.919 (1.558)\t Loss 74.917 (93.564)\t EPE 13.847 (17.594)\n",
            "Epoch: [23][150/154]\t Time 0.660 (1.692)\t Data 0.594 (1.558)\t Loss 83.924 (93.690)\t EPE 16.036 (17.619)\n",
            "Test: [0/34]\t Time 1.014 (1.014)\t EPE 15.941 (15.941)\n",
            "Test: [10/34]\t Time 0.620 (0.408)\t EPE 14.070 (12.779)\n",
            "Test: [20/34]\t Time 0.341 (0.333)\t EPE 15.637 (12.861)\n",
            "Test: [30/34]\t Time 0.284 (0.290)\t EPE 14.451 (12.144)\n",
            " * EPE 12.193\n",
            "Epoch: [24][0/154]\t Time 3.057 (3.057)\t Data 2.911 (2.911)\t Loss 84.794 (84.794)\t EPE 16.527 (16.527)\n",
            "Epoch: [24][10/154]\t Time 2.282 (1.782)\t Data 2.158 (1.657)\t Loss 108.603 (99.068)\t EPE 21.091 (18.900)\n",
            "Epoch: [24][20/154]\t Time 1.858 (1.719)\t Data 1.798 (1.608)\t Loss 99.734 (97.212)\t EPE 19.622 (18.684)\n",
            "Epoch: [24][30/154]\t Time 1.543 (1.696)\t Data 1.477 (1.594)\t Loss 97.039 (93.730)\t EPE 18.898 (17.898)\n",
            "Epoch: [24][40/154]\t Time 1.665 (1.691)\t Data 1.593 (1.593)\t Loss 95.127 (92.825)\t EPE 18.779 (17.785)\n",
            "Epoch: [24][50/154]\t Time 1.822 (1.685)\t Data 1.758 (1.591)\t Loss 81.571 (93.339)\t EPE 15.194 (17.840)\n",
            "Epoch: [24][60/154]\t Time 4.144 (1.743)\t Data 4.014 (1.647)\t Loss 82.577 (93.790)\t EPE 15.386 (17.901)\n",
            "Epoch: [24][70/154]\t Time 3.863 (1.730)\t Data 3.713 (1.634)\t Loss 92.044 (93.338)\t EPE 17.735 (17.821)\n",
            "Epoch: [24][80/154]\t Time 3.363 (1.713)\t Data 3.240 (1.616)\t Loss 105.783 (93.451)\t EPE 20.229 (17.773)\n",
            "Epoch: [24][90/154]\t Time 2.244 (1.696)\t Data 2.131 (1.600)\t Loss 83.670 (93.990)\t EPE 16.477 (17.857)\n",
            "Epoch: [24][100/154]\t Time 1.613 (1.692)\t Data 1.548 (1.597)\t Loss 126.211 (94.753)\t EPE 24.825 (17.974)\n",
            "Epoch: [24][110/154]\t Time 1.723 (1.693)\t Data 1.645 (1.599)\t Loss 88.677 (94.368)\t EPE 16.330 (17.888)\n",
            "Epoch: [24][120/154]\t Time 1.653 (1.691)\t Data 1.580 (1.598)\t Loss 95.787 (94.381)\t EPE 17.314 (17.888)\n",
            "Epoch: [24][130/154]\t Time 1.614 (1.690)\t Data 1.543 (1.597)\t Loss 79.432 (93.702)\t EPE 15.779 (17.787)\n",
            "Epoch: [24][140/154]\t Time 1.453 (1.689)\t Data 1.391 (1.597)\t Loss 90.238 (93.989)\t EPE 16.366 (17.869)\n",
            "Epoch: [24][150/154]\t Time 1.313 (1.688)\t Data 1.252 (1.597)\t Loss 114.744 (94.157)\t EPE 21.754 (17.928)\n",
            "Test: [0/34]\t Time 0.675 (0.675)\t EPE 16.076 (16.076)\n",
            "Test: [10/34]\t Time 0.339 (0.244)\t EPE 13.885 (12.558)\n",
            "Test: [20/34]\t Time 0.626 (0.293)\t EPE 17.179 (12.703)\n",
            "Test: [30/34]\t Time 0.473 (0.302)\t EPE 14.545 (12.029)\n",
            " * EPE 12.036\n",
            "Epoch: [25][0/154]\t Time 3.021 (3.021)\t Data 2.880 (2.880)\t Loss 102.553 (102.553)\t EPE 19.548 (19.548)\n",
            "Epoch: [25][10/154]\t Time 2.371 (1.799)\t Data 2.261 (1.652)\t Loss 111.666 (89.555)\t EPE 22.280 (17.266)\n",
            "Epoch: [25][20/154]\t Time 3.066 (1.749)\t Data 2.946 (1.621)\t Loss 93.564 (89.866)\t EPE 18.123 (17.075)\n",
            "Epoch: [25][30/154]\t Time 3.356 (1.732)\t Data 3.237 (1.586)\t Loss 98.277 (91.731)\t EPE 19.140 (17.524)\n",
            "Epoch: [25][40/154]\t Time 4.043 (1.725)\t Data 3.911 (1.572)\t Loss 92.299 (92.266)\t EPE 18.221 (17.600)\n",
            "Epoch: [25][50/154]\t Time 4.135 (1.716)\t Data 3.988 (1.557)\t Loss 79.569 (91.349)\t EPE 14.960 (17.398)\n",
            "Epoch: [25][60/154]\t Time 4.619 (1.704)\t Data 4.460 (1.542)\t Loss 107.472 (91.609)\t EPE 20.272 (17.454)\n",
            "Epoch: [25][70/154]\t Time 2.823 (1.767)\t Data 2.700 (1.612)\t Loss 92.028 (92.637)\t EPE 17.023 (17.591)\n",
            "Epoch: [25][80/154]\t Time 3.494 (1.761)\t Data 3.373 (1.605)\t Loss 95.064 (92.986)\t EPE 18.635 (17.657)\n",
            "Epoch: [25][90/154]\t Time 3.762 (1.751)\t Data 3.641 (1.593)\t Loss 96.295 (93.875)\t EPE 18.246 (17.784)\n",
            "Epoch: [25][100/154]\t Time 4.374 (1.741)\t Data 4.250 (1.588)\t Loss 76.522 (93.860)\t EPE 14.254 (17.767)\n",
            "Epoch: [25][110/154]\t Time 4.294 (1.727)\t Data 4.143 (1.579)\t Loss 78.072 (93.794)\t EPE 15.302 (17.756)\n",
            "Epoch: [25][120/154]\t Time 4.070 (1.716)\t Data 3.899 (1.571)\t Loss 101.400 (94.674)\t EPE 18.914 (17.889)\n",
            "Epoch: [25][130/154]\t Time 3.855 (1.715)\t Data 3.722 (1.568)\t Loss 89.336 (94.356)\t EPE 16.243 (17.822)\n",
            "Epoch: [25][140/154]\t Time 3.106 (1.708)\t Data 2.954 (1.559)\t Loss 90.168 (94.713)\t EPE 17.076 (17.857)\n",
            "Epoch: [25][150/154]\t Time 2.390 (1.702)\t Data 2.273 (1.550)\t Loss 87.452 (94.651)\t EPE 16.286 (17.853)\n",
            "Test: [0/34]\t Time 1.274 (1.274)\t EPE 15.909 (15.909)\n",
            "Test: [10/34]\t Time 0.350 (0.361)\t EPE 13.864 (12.597)\n",
            "Test: [20/34]\t Time 0.237 (0.274)\t EPE 15.867 (12.619)\n",
            "Test: [30/34]\t Time 0.255 (0.249)\t EPE 14.449 (11.913)\n",
            " * EPE 11.945\n",
            "Epoch: [26][0/154]\t Time 3.065 (3.065)\t Data 2.915 (2.915)\t Loss 112.385 (112.385)\t EPE 20.300 (20.300)\n",
            "Epoch: [26][10/154]\t Time 2.467 (1.815)\t Data 2.342 (1.648)\t Loss 94.175 (99.005)\t EPE 18.892 (18.255)\n",
            "Epoch: [26][20/154]\t Time 2.417 (1.754)\t Data 2.294 (1.581)\t Loss 106.561 (95.122)\t EPE 20.162 (17.926)\n",
            "Epoch: [26][30/154]\t Time 2.547 (1.724)\t Data 2.436 (1.550)\t Loss 100.263 (98.690)\t EPE 19.963 (18.775)\n",
            "Epoch: [26][40/154]\t Time 2.451 (1.721)\t Data 2.333 (1.544)\t Loss 93.903 (96.521)\t EPE 16.646 (18.335)\n",
            "Epoch: [26][50/154]\t Time 2.492 (1.716)\t Data 2.383 (1.539)\t Loss 78.750 (94.383)\t EPE 15.396 (17.927)\n",
            "Epoch: [26][60/154]\t Time 4.415 (1.747)\t Data 4.239 (1.569)\t Loss 78.757 (93.595)\t EPE 15.954 (17.867)\n",
            "Epoch: [26][70/154]\t Time 2.590 (1.769)\t Data 2.463 (1.590)\t Loss 102.163 (93.367)\t EPE 20.421 (17.852)\n",
            "Epoch: [26][80/154]\t Time 2.597 (1.763)\t Data 2.480 (1.584)\t Loss 82.973 (93.494)\t EPE 14.167 (17.837)\n",
            "Epoch: [26][90/154]\t Time 2.511 (1.757)\t Data 2.401 (1.579)\t Loss 110.058 (93.041)\t EPE 20.229 (17.734)\n",
            "Epoch: [26][100/154]\t Time 2.491 (1.748)\t Data 2.366 (1.570)\t Loss 69.154 (92.884)\t EPE 12.924 (17.738)\n",
            "Epoch: [26][110/154]\t Time 2.528 (1.742)\t Data 2.412 (1.563)\t Loss 85.960 (93.311)\t EPE 14.537 (17.792)\n",
            "Epoch: [26][120/154]\t Time 2.433 (1.735)\t Data 2.313 (1.556)\t Loss 96.937 (93.441)\t EPE 19.046 (17.823)\n",
            "Epoch: [26][130/154]\t Time 2.518 (1.733)\t Data 2.404 (1.554)\t Loss 95.449 (93.078)\t EPE 18.017 (17.741)\n",
            "Epoch: [26][140/154]\t Time 3.131 (1.729)\t Data 3.020 (1.551)\t Loss 103.495 (92.863)\t EPE 20.508 (17.712)\n",
            "Epoch: [26][150/154]\t Time 3.752 (1.726)\t Data 3.642 (1.548)\t Loss 84.068 (92.765)\t EPE 15.846 (17.709)\n",
            "Test: [0/34]\t Time 0.713 (0.713)\t EPE 16.148 (16.148)\n",
            "Test: [10/34]\t Time 0.357 (0.249)\t EPE 13.868 (12.594)\n",
            "Test: [20/34]\t Time 0.268 (0.227)\t EPE 16.691 (12.661)\n",
            "Test: [30/34]\t Time 0.334 (0.217)\t EPE 14.366 (11.983)\n",
            " * EPE 11.984\n",
            "Epoch: [27][0/154]\t Time 5.482 (5.482)\t Data 5.312 (5.312)\t Loss 82.638 (82.638)\t EPE 15.467 (15.467)\n",
            "Epoch: [27][10/154]\t Time 4.720 (1.994)\t Data 4.589 (1.819)\t Loss 78.376 (92.822)\t EPE 15.179 (17.779)\n",
            "Epoch: [27][20/154]\t Time 4.124 (1.819)\t Data 3.999 (1.644)\t Loss 88.288 (88.444)\t EPE 15.722 (16.861)\n",
            "Epoch: [27][30/154]\t Time 3.340 (1.756)\t Data 3.222 (1.581)\t Loss 88.998 (89.551)\t EPE 16.560 (17.072)\n",
            "Epoch: [27][40/154]\t Time 2.779 (1.722)\t Data 2.648 (1.546)\t Loss 83.777 (91.498)\t EPE 15.822 (17.501)\n",
            "Epoch: [27][50/154]\t Time 2.523 (1.710)\t Data 2.411 (1.535)\t Loss 82.050 (90.853)\t EPE 15.350 (17.365)\n",
            "Epoch: [27][60/154]\t Time 2.553 (1.709)\t Data 2.433 (1.534)\t Loss 86.675 (91.424)\t EPE 16.595 (17.478)\n",
            "Epoch: [27][70/154]\t Time 4.854 (1.791)\t Data 4.727 (1.613)\t Loss 80.046 (92.531)\t EPE 15.555 (17.719)\n",
            "Epoch: [27][80/154]\t Time 3.308 (1.765)\t Data 3.181 (1.587)\t Loss 90.137 (91.817)\t EPE 17.675 (17.610)\n",
            "Epoch: [27][90/154]\t Time 2.648 (1.748)\t Data 2.517 (1.570)\t Loss 112.630 (91.958)\t EPE 22.836 (17.683)\n",
            "Epoch: [27][100/154]\t Time 2.529 (1.742)\t Data 2.421 (1.564)\t Loss 127.663 (92.283)\t EPE 24.482 (17.765)\n",
            "Epoch: [27][110/154]\t Time 2.503 (1.737)\t Data 2.381 (1.558)\t Loss 91.729 (91.811)\t EPE 17.916 (17.669)\n",
            "Epoch: [27][120/154]\t Time 2.408 (1.732)\t Data 2.287 (1.554)\t Loss 123.664 (92.094)\t EPE 23.965 (17.735)\n",
            "Epoch: [27][130/154]\t Time 2.434 (1.728)\t Data 2.312 (1.549)\t Loss 77.718 (91.972)\t EPE 15.210 (17.730)\n",
            "Epoch: [27][140/154]\t Time 2.465 (1.724)\t Data 2.356 (1.545)\t Loss 116.710 (91.743)\t EPE 22.297 (17.674)\n",
            "Epoch: [27][150/154]\t Time 2.538 (1.722)\t Data 2.412 (1.543)\t Loss 78.727 (91.711)\t EPE 15.565 (17.674)\n",
            "Test: [0/34]\t Time 0.713 (0.713)\t EPE 16.070 (16.070)\n",
            "Test: [10/34]\t Time 0.261 (0.239)\t EPE 13.819 (12.495)\n",
            "Test: [20/34]\t Time 0.477 (0.284)\t EPE 17.039 (12.613)\n",
            "Test: [30/34]\t Time 0.634 (0.303)\t EPE 14.468 (11.928)\n",
            " * EPE 11.935\n",
            "Epoch: [28][0/154]\t Time 3.069 (3.069)\t Data 2.915 (2.915)\t Loss 107.922 (107.922)\t EPE 20.407 (20.407)\n",
            "Epoch: [28][10/154]\t Time 2.511 (1.781)\t Data 2.393 (1.627)\t Loss 97.399 (95.965)\t EPE 17.005 (17.892)\n",
            "Epoch: [28][20/154]\t Time 3.304 (1.738)\t Data 3.189 (1.580)\t Loss 114.072 (93.072)\t EPE 23.289 (17.515)\n",
            "Epoch: [28][30/154]\t Time 3.713 (1.720)\t Data 3.593 (1.555)\t Loss 100.422 (93.815)\t EPE 18.980 (17.803)\n",
            "Epoch: [28][40/154]\t Time 4.376 (1.718)\t Data 4.254 (1.550)\t Loss 78.840 (93.975)\t EPE 15.445 (17.931)\n",
            "Epoch: [28][50/154]\t Time 4.323 (1.712)\t Data 4.165 (1.541)\t Loss 136.261 (94.623)\t EPE 27.112 (18.140)\n",
            "Epoch: [28][60/154]\t Time 4.575 (1.699)\t Data 4.447 (1.526)\t Loss 88.194 (94.678)\t EPE 17.313 (18.136)\n",
            "Epoch: [28][70/154]\t Time 4.432 (1.687)\t Data 4.313 (1.515)\t Loss 71.930 (95.643)\t EPE 13.574 (18.346)\n",
            "Epoch: [28][80/154]\t Time 2.967 (1.754)\t Data 2.839 (1.579)\t Loss 97.213 (95.748)\t EPE 17.702 (18.300)\n",
            "Epoch: [28][90/154]\t Time 3.538 (1.753)\t Data 3.418 (1.578)\t Loss 104.457 (95.825)\t EPE 20.348 (18.307)\n",
            "Epoch: [28][100/154]\t Time 3.887 (1.750)\t Data 3.759 (1.574)\t Loss 82.566 (95.519)\t EPE 16.015 (18.249)\n",
            "Epoch: [28][110/154]\t Time 4.131 (1.745)\t Data 4.008 (1.569)\t Loss 93.896 (95.198)\t EPE 17.952 (18.202)\n",
            "Epoch: [28][120/154]\t Time 4.916 (1.744)\t Data 4.801 (1.568)\t Loss 72.693 (94.504)\t EPE 12.597 (18.053)\n",
            "Epoch: [28][130/154]\t Time 4.524 (1.737)\t Data 4.390 (1.560)\t Loss 61.331 (93.455)\t EPE 11.668 (17.865)\n",
            "Epoch: [28][140/154]\t Time 4.607 (1.728)\t Data 4.486 (1.551)\t Loss 70.218 (92.896)\t EPE 13.644 (17.781)\n",
            "Epoch: [28][150/154]\t Time 3.486 (1.717)\t Data 3.366 (1.541)\t Loss 111.998 (92.625)\t EPE 21.874 (17.738)\n",
            "Test: [0/34]\t Time 0.952 (0.952)\t EPE 15.957 (15.957)\n",
            "Test: [10/34]\t Time 0.322 (0.268)\t EPE 14.086 (12.635)\n",
            "Test: [20/34]\t Time 0.080 (0.229)\t EPE 16.226 (12.682)\n",
            "Test: [30/34]\t Time 0.094 (0.223)\t EPE 14.600 (12.032)\n",
            " * EPE 12.062\n",
            "Epoch: [29][0/154]\t Time 3.001 (3.001)\t Data 2.862 (2.862)\t Loss 73.928 (73.928)\t EPE 14.595 (14.595)\n",
            "Epoch: [29][10/154]\t Time 2.474 (1.838)\t Data 2.351 (1.660)\t Loss 107.098 (87.894)\t EPE 20.611 (16.585)\n",
            "Epoch: [29][20/154]\t Time 2.519 (1.765)\t Data 2.394 (1.587)\t Loss 105.395 (94.484)\t EPE 18.892 (17.960)\n",
            "Epoch: [29][30/154]\t Time 2.165 (1.726)\t Data 2.056 (1.559)\t Loss 88.346 (91.738)\t EPE 16.457 (17.394)\n",
            "Epoch: [29][40/154]\t Time 2.165 (1.716)\t Data 2.036 (1.565)\t Loss 113.727 (90.959)\t EPE 21.003 (17.287)\n",
            "Epoch: [29][50/154]\t Time 1.993 (1.704)\t Data 1.926 (1.562)\t Loss 76.459 (90.165)\t EPE 14.016 (17.117)\n",
            "Epoch: [29][60/154]\t Time 2.062 (1.699)\t Data 1.976 (1.565)\t Loss 85.268 (89.769)\t EPE 16.158 (17.024)\n",
            "Epoch: [29][70/154]\t Time 1.951 (1.693)\t Data 1.872 (1.564)\t Loss 105.609 (91.167)\t EPE 20.593 (17.319)\n",
            "Epoch: [29][80/154]\t Time 2.305 (1.689)\t Data 2.188 (1.565)\t Loss 81.608 (91.721)\t EPE 15.699 (17.446)\n",
            "Epoch: [29][90/154]\t Time 3.179 (1.734)\t Data 3.067 (1.612)\t Loss 78.376 (91.234)\t EPE 15.436 (17.384)\n",
            "Epoch: [29][100/154]\t Time 3.590 (1.721)\t Data 3.454 (1.601)\t Loss 102.865 (90.599)\t EPE 19.228 (17.278)\n",
            "Epoch: [29][110/154]\t Time 3.930 (1.708)\t Data 3.812 (1.591)\t Loss 71.342 (91.035)\t EPE 12.964 (17.359)\n",
            "Epoch: [29][120/154]\t Time 2.736 (1.695)\t Data 2.614 (1.581)\t Loss 75.795 (91.009)\t EPE 15.162 (17.358)\n",
            "Epoch: [29][130/154]\t Time 1.974 (1.687)\t Data 1.832 (1.574)\t Loss 82.454 (90.685)\t EPE 16.415 (17.308)\n",
            "Epoch: [29][140/154]\t Time 1.544 (1.683)\t Data 1.482 (1.571)\t Loss 102.206 (90.839)\t EPE 20.919 (17.321)\n",
            "Epoch: [29][150/154]\t Time 1.016 (1.679)\t Data 0.938 (1.569)\t Loss 82.287 (91.158)\t EPE 16.093 (17.397)\n",
            "Test: [0/34]\t Time 0.836 (0.836)\t EPE 16.380 (16.380)\n",
            "Test: [10/34]\t Time 0.234 (0.381)\t EPE 13.809 (12.622)\n",
            "Test: [20/34]\t Time 0.120 (0.341)\t EPE 16.868 (12.818)\n",
            "Test: [30/34]\t Time 0.097 (0.296)\t EPE 14.268 (12.078)\n",
            " * EPE 12.096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate code"
      ],
      "metadata": {
        "id": "opTPtrU16fZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_EPE = validate(val_loader, model, 0, output_writers)"
      ],
      "metadata": {
        "id": "Hxb_wT9J6kfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff55f8f1-8566-41fd-f97f-01b154e6886e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: [0/34]\t Time 0.681 (0.681)\t EPE 16.380 (16.380)\n",
            "Test: [10/34]\t Time 0.216 (0.320)\t EPE 13.809 (12.622)\n",
            "Test: [20/34]\t Time 0.135 (0.262)\t EPE 16.868 (12.818)\n",
            "Test: [30/34]\t Time 0.178 (0.248)\t EPE 14.268 (12.078)\n",
            " * EPE 12.096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model on disk"
      ],
      "metadata": {
        "id": "8WQ3ENEsmH-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cp ./model_/model_best.pth.tar ./drive/MyDrive/flownets.pth.tar"
      ],
      "metadata": {
        "id": "36bG-8YlmPv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}